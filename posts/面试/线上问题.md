# 线上问题

## 线程池使用不当导致的生产故障

### 背景

`product-search-center`​服务突然间频繁告警，rt飙升，随后pod重启，随后紧急联系运维，扩容，短暂救回来了。。。

### 诊断病因

“病人” 是暂时救回来了，但病因还未找到，随时都有可能再次陷入危急。到公司后，我便马不停蹄的开始定位本次故障的根因。

好消息是，在前面应急的过程中，已经掌握了不少有用的信息：

1. 故障发生时 仅有 2 个 Pod 节点；
2. 故障发生时，该接口的 QPS 是前一天同时段的约 2 倍

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185727020.png" alt="image-20250614185727020" style="zoom:40%;" />

3. 初步判断，故障可能是 依赖的一个下游接口（以下称作 getSkuInfo）超时抖动引起的（RT 上涨到了 500ms，正常情况下 P95 在 10ms 左右，持续约 1s）；

综合这些信息来看，Pod 节点数过少 + 切流导致流量增加 + 依赖耗时突发抖动，且最终通过扩容得以恢复，这叠满的 buff， 将故障原因指向了   **“容量不足”**  。

#### 初步定位异常指征：tomcat 线程池处理能力饱和，任务排队

通过 Pod 监控，轻易就能排除算力、存储等硬件资源耗尽的可能性。因此，最有可能还是工作线程出了问题。

> 两个 Pod 的 CPU 利用率、内存利用率等指标都处于低水位。

我们使用 tomcat 线程池来处理请求，工作线程的使用情况可以通过 “tomcat 线程池监控” 获得。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185743950.png" alt="image-20250614185743950" style="zoom:50%;" />

#### 线程池处理能力饱和的后果：任务排队导致探活失败，引发 Pod 重启

当前，该应用基于默认配置，使用 SpringBoot 的健康检查端口（Endpoint），即`actuator/health`​，作为容器的存活探针。而问题就有可能出在这个存活探针上，k8s 会请求应用的`actuator/health`​端口，而这个请求也需要提交给 tomcat 线程池执行。

设想如**下图**中的场景：

Thread-1 ～ Thread-200 因处理业务请求而饱和，工作队列已经积压了一些待处理的业务请求（待处理任务 R1 ～ R4），此时 k8s 发出了探活请求（R5），但只能在队列中等待。对于任务 R5 来说，最好的情况是刚刚放入工作队列就立刻有$\ge$5个工作线程从之前的任务中释放出来，R1～R5 被并行取出执行，探活请求被立即处理。但线程池因业务请求饱和时，并不存在这种理想情况。通过前文已经知道，confirmEvaluate 的耗时飙升到了 5s，排在 R5 前面的业务请求处理都很慢。因此比较坏的情况是，R5 需要等待 R1、R2、R3、R4 依次执行完成才能获得工作线程,将在任务队列中积压 20s， 甚至更久。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185800230.png" alt="image-20250614185800230" style="zoom:70%;" />

而 该服务 的探活超时时间设置的是 1s，一旦发生积压的情况，则大概率会超时。此时 k8s 会接收到超时异常，因此判定探活失败。

果然，通过查阅 Pod 事件日志，我发现 Pod-1（ *.* .186.8）在 08:53:59 记录了探活失败，随后触发了重启，Pod-2（ *.* .188.173）则是在 08:53:33 记录了探活失败，随后也触发了重启。而这两个时间正是在上文提到的  **“线程池达到饱和&quot;**  的两个时间点附近（Pod-1 08:54:00 和 Pod-2 00:53:30）。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185819480.png" alt="image-20250614185819480" style="zoom:30%;" />

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185833250.png" alt="image-20250614185833250" style="zoom:30%;" />

#### 深度检查：寻找线程池处理能力恶化的根因

现在，已经明确了 “tomcat 线程池饱和” 是导致这次容量问题的关键，但线程池为什么会饱和，还需要继续寻找原因。

结合应急处置过程中获得的 #3 线索和过往经验，首先浮现在我脑中的推论是：SOA 调用下游 getSkuInfo 耗时增加，导致 tomcat 线程陆续陷入等待，最终耗尽了所有可用线程。

##### 与推论相矛盾的关键证据：`WAITING`​状态线程数飙升

下图是两个 Pod 在故障时段的线程状态监控曲线（X 轴为时间，Y 轴为线程的数量）。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185852454.png" alt="image-20250614185852454" style="zoom:50%;" />

可以看到，不论 Pod-1（ *.* .186.8）还是 Pod-2（ *.* .188.173）在故障开始之初（08:52:00$\pm$30s），都是`WAITING`​状态的线程显著飙升。这有点出乎意料，因为如果开头的推测成立，下游 getTagInfo 耗时增加，线程的波动应该体现为`TIMED_WAITING`​ 状态的线程数飙升。

##### 是推论站不住脚，还是错误理解了证据？深度溯源框架代码，拨开迷雾

一次标准的 SOA 调用，并不会导致 tomcat 线程进入 `WAITING`​ 状态。

虽然 tomcat 线程确实会在 HTTP 请求执行完成前，一直等待 `future.get(Integer.MAX_VALUE, TimeUnit.MILLISECONDS)`​ 返回，随着 SOA 调用耗时增加，等待的时间也会更久，从而导致可用线程逐渐耗尽。但正如最初的设想一样，此时应该 `TIMED_WAITING `​状态的线程数飙升，与实际监控曲线的特征不符；

##### 弄巧成拙的业务代码

既然已经意识到 `WAITING`​ 状态线程数飙升的事实，不如基于这个事实给问题代码做个画像。那么，什么样的代码可能会导致线程进入`WAITING`​状态呢？

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185905956.png" alt="image-20250614185905956" style="zoom:50%;" />

或者，也有可能是执行了将这些方法进行过上层包装后的方法。例如，

- 当执行 CompletableFuture 实例的 `future.get(timeout, unit) `​方法时，在 CompletableFuture 的底层实现中，最终会调用` `​LockSupport.parkNanos(o, timeout)`方法；
- 而如果执行 CompletableFuture 实例的 `future.get() `​方法（不指定超时时间），则最终会调用 `LockSupport.park(o)`​ 方法；

因此，执行 `future.get(timeout, unit) `​方法将导致线程进入 `TIMED_WAITING `​状态（正如前文讨论的框架代码一样），而执行 `future.get() `​方法则会导致线程进入` WAITING`​ 状态。

尽管 该服务 的定位是业务聚合层的应用，主要是串行调用下游领域服务，编排产品功能，通常不太可能主动使用上面提到的这些方法来管理线程。但毕竟已经确定了框架代码中并无任何导致`WAITING`​状态线程数飙升的证据，因此我还是决定到业务代码中一探究竟。

没想到很快就发现了蛛丝马迹。几乎没废什么功夫，就从 confirmEvaluate 方法中找到了一段高度符合画像的代码，经过精简后，如**图23**。当看到第 15 行和第 16 行的 `future.get()`​ 调用时，我不禁眉头一紧。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185918383.png" alt="image-20250614185918383" style="zoom:60%;" />

和当时开发这段代码的同学了解了一下， 创作背景大致是这样的：

> 由于种种历史原因，在确认页估价时，需要根据 “是否到付”，分两次调用下游进行估价（即**图23**中第6行g`etArrivalPayConfirmEvaluateResult`​ 和 第 10 行 `getCommonConfirmEvaluateResult）`​。confirmEvaluate 的耗时主要受调用下游计价服务的耗时影响，原先日常 P95 约为 750ms，串行调用两次会导致 confirmEvaluate 方法的耗时 double，达到 1.5s 左右。考虑到用户体验，于是决定通过多线程并发执行来减少耗时。

虽然想法无可厚非，但具体到代码实现却有几个明显的问题：

1. 在调用 `future.get()`​ 等待线程返回结果时，没有设置超时时间，这是非常不可取的错误实践；
2. `getArrivalPayConfirmEvaluateResult`​ 和 `getCommonConfirmEvaluateResult`​ 中包装了繁重的业务逻辑（且重复性极高），不仅会调用下游的计价服务，还会调用包括 getTagInfo 在内的多个其他下游服务的接口。而最初的目的仅仅是解决 “调用两次计价服务” 的问题；
3. 使用了自定义线程池 `BizExecutorsUtils.EXECUTOR`​ 而不是框架推荐的动态线程池；

尤其是看到使用了自定义线程池 `BizExecutorsUtils.EXECUTOR`​ 的时候，我第二次眉头一紧，心中预感问题找到了。`BizExecutorsUtils.EXECUTOR`​ 的定义如**图24**。

1. **Line 16**：可以看到线程池的最大线程数只有 20；
2. **Line 19**：工作队列却很大，可以允许 1K+ 个任务排队；

此外，一个平常看起来合情合理，但用在这里却雪上加霜的设计是，`EXECUTOR`​ 是静态初始化的，在同一个 JVM 进程中全局唯一。这里的线程池定义，很难不让我想到只有 3 个诊室，却排了 500 号病人的呼吸内科。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185935107.png" alt="image-20250614185935107" style="zoom:70%;" />

正是因为这个线程池的引入，导致了 “瓶颈效应”。

故障发生时，tomcat 线程池配置的最大线程数为 200（单个 Pod 最多能够并发处理 200 个请求）。但在方法内部，估价的核心逻辑被包装成了两个子任务，提交到了最大仅有 20 个工作线程的 EXECUTOR 自定义线程池执行。也就是说，EXECUTOR 自定义线程池的处理能力仅有 tomcat 线程池的 1/10，却需要处理 2 倍于 QPS 的任务量。

我们不妨结合前文的信息做个粗略的估算：

1. 首先，在 getTagInfo 耗时抖动前，请求量约为 75+ requests/s，负载均衡到 2 个 zone 的共 4 个 Pod 上，单个 Pod 的请求量约为 18 ～ 20 requests/s；
2. 在 getSkuInfo 耗时抖动前，一次估价流程的耗时约为 750ms，getTagInfo 耗时飙升后（10ms → 500ms），一次估价流程的耗时理论上增加到 1.3s 左右。由于 confirmEvaluate 内部拆分的子任务，几乎就是完整的估价流程，因此，两个子任务的执行耗时也需要 1.3s 左右；
3. 在 getSkuInfo 耗时抖动的 1s 内，虽然请求量无显著变化，但根据 2 倍关系，这 1s 内将会产生 36 ～ 40 个任务（单个 Pod），提交给 EXECUTOR 线程池执行；
4. 由于单个子任务执行耗时 $>$1s，也就是说，在这 1s 内提交到 EXECUTOR 线程池的任务都无法执行完，EXECUTOR 的 20 个工作线程，将全部饱和，大约有一半的任务正占用工作线程，另一半的任务在工作队列中等待；

显然，这是一个十分简单的计算模型，并不能反映现实中更加复杂的情况$^{[12]}$，但并不妨碍说明问题（和 2.2 节中分析的案例一模一样的问题）。子任务可能在 EXECUTOR 线程池工作队列中排队很久， 从而导致 confirmEvaluate 方法在 `future.get()`​ 上等待（**图23**第15、16行），一直占用 tomcat 工作线程，新到达的请求只能不断创建新的 tomcat 线程来执行。同时，confirmEvaluate 响应变慢或超时错误，又会诱发用户重试，进一步带来请求量激增（如**图1**，抖动发生后，请求量飙升到了约 100 requests/s），导致 EXECUTOR 线程池的处理能力进一步恶化。如此恶性循环，最终彻底耗尽 tomcat 线程池的可用工作线程。

通过 Trace 跟踪，我们可以轻松找出支持 “瓶颈效应” 的证据，如**图**。confirmEvaluate 方法首先会调用下游的 get_user_info 接口获取用户信息$^{[13]}$。然后，该方法将构建两个子任务并将其提交至 EXECUTOR 线程池（**图** 第 5～7 行、 第 9～11 行）。值得注意的是，get_user_info 调用完成后，到子任务被提交到 EXECUTOR 线程池的这段时间里，代码只做了一些参数包装和验证的简单逻辑辑，因此这部分代码的执行时间基本可以忽略不计。我们可以近似的理解为，在 get_user_info 的调用完成后，子任务就立即提交给了 EXECUTOR 线程池。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185957440.png" alt="image-20250614185957440" style="zoom:25%;" />

当 EXECUTOR 线程池处理能力足够时，子任务被提交到 EXECUTOR 线程池后也应该迅速被执行，因此从 get_user_info 接口调用完成到子任务开始执行中间的间隔时间应该很短。但从 **图**中给出的 Trace 链路可以看到，从 get_user_info 接口调用，到子任务 1 和子任务 2 开始执行，间隔了约 6s 的时间$^{[15]}$。而这 6s 反映的正是子任务在 EXECUTOR 线程池中等待执行的耗时。

#### 诊断结论

至此，终于可以给出诊断结论：

|根因|业务代码中不合理使用自定义线程池导致的 “瓶口效应”。|
| ------| -------------------------------------------------------|
|**诱因**|**下游 getTagInfo 接口耗时抖动**|
|**其他次要因素**|**1.  切流导致流量增加；2.Pod 数量少；3.探活机制脆弱；4.用户重试导致流量异常增涨;**|
|**结论**|**业务代码中不合理使用自定义线程池导致 “瓶口效应”，间接限制了 tomcat 的吞吐能力。10 月 26 日晚切流后，请求量达到前一天的两倍，自定义线程池处理能力已经接近瓶颈（20 task/s）。碰巧遇到 10 月 27 日早晨 getTagInfo 耗时抖动，子任务执行耗时显著增加，自定义线程池处理能力迅速触动崩溃，进而导致 tomcat 处理请求排队。长时间积压在 tomcat 线程池工作队列中的探活请求超时，导致探活失败，触发 k8s 先后重启了 zone-2 仅有的 2 个 Pod，最终导致整个 zone-2 不可用。**|