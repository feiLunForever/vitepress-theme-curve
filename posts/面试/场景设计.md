# 场景设计

## 限流

### 多机版

- **Hystrix**

    - Hystrix 是一个断路器模式的开源实现，它提供了线程隔离、熔断和限流等功能。
    - Hystrix 中的限流功能通过控制请求队列的大小来实现，如果请求队列满了，后续请求会被拒绝。

### 单机版

#### 计数器限流

- 设置阈值，最大处理数量，阈值 与 处理中的请求 对比
- AtomicInteger 计数
- 分布式 redis

#### 滑动窗口限流

- 参照 tcp 滑动窗口

    - 分为发送窗口、接收窗口
    - 接收方通过数据包发送方自己当前窗口大小和期望接收到的下一字节的序号 n，从而控制发送方的发送速度，实现对网络传输流量的控制，防止发送方速度过快导致自己被淹没

#### 漏桶限流

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184418066.png" alt="image-20250614184418066" style="zoom:30%;" />

- 流量生产端和接收端之间增加了一个漏桶，流量会先暂存在漏桶中
- 超过了漏桶的部分会被拒绝服务
- 漏桶的出口处会按照一个固定的速率将流量漏出给消费端

#### 令牌桶限流

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184435476.png" alt="image-20250614184435476" style="zoom:40%;" />

- Guava 包中的 RateLimiter
- 如果限流规则是 1 秒钟 1000 次请求，那么每隔 1 毫秒，流量生产端会往桶中放入一个令牌
- 接收端在处理一个请求之前，先从桶中取一个令牌，才能继续后续的业务处理。如果桶中没有令牌，则需要等待或者拒绝服务
- 初始化时，会限制桶中的令牌总数，如果超过了限制数就不再向桶中放入新的令牌
- 如果是分布式环境，令牌存储在 Redis 中（incr 计数），每次处理请求前访问一次 Redis 获取一个令牌

## 外部接口大量超时，把整个系统拖垮，引发雪崩！如何解决？熔断...

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184453390.png" alt="image-20250614184453390" style="zoom:40%;" />

**举个例子：**

- Service D 挂了，响应很慢
- Service G 和 Service F ，都依赖 Service D，也会受到牵连，对外响应也会变慢
- 影响层层向上传递，Service A 和 Service B 也会被拖垮
- 最后，引发雪崩效应，系统的故障影响面会越来越大

为了解决这种问题，我们需要引入 `熔断`​ 机制。

### 什么是熔断？

熔断，其实是对调用链路中某个资源出现不稳定状态时（如：调用超时或异常比例升高），对这个资源的调用进行限制，让请求快速失败，避免影响到其它的资源而导致级联错误。

当资源被降级后，在接下来的`降级时间窗口`​内，对该资源的调用都自动熔断（默认是抛出 `BlockException`​）

目前市面上的熔断框架很多，如：`Sentinel`​、`Hystrix`​、`Resilience4j`​ 等，这些框架的设计理念都差不多。

### 那如何提高整体的可用性呢？

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184509665.png" alt="image-20250614184509665" style="zoom:70%;" />

#### 限流 - 自知之明和眼力见

##### 被动限流

系统对自身的承载能力需要有一个清晰的认识，对于超过承载能力的额外调用，要适当拒绝。

而怎样衡量系统承载能力是一个问题。

一般的我们有两种常见方案：一是**定义阈值和规则**，二是**自适应限流策略**。

> - 阈值和规则是owner通过对业务的把控和自身的存储、连接的现状，根据人工经验制定的。这样的策略一般不会出什么大问题，但是不够灵活，对请求反馈的灵敏度和资源的利用率不够。
>
> - 相对的，自适应策略则是一种动态限流策略，是通过对系统当前的运行状况，动态的调整限流阈值，在机器资源和流量处理之间寻找一个平衡。

##### 主动限流

对下游依赖系统的服务能力，需要有一个精准的判断，对于服务能力弱的下游系统，要适当减少调用，得有点眼力见，对不对。

因为，绝大部分的业务系统都不是单独存在的，会依赖很多其他的系统，这些依赖方的服务能力，就像是木桶短板，限制了当前系统的处理能力。这个时候就需要把下游当做一个整体来考虑。

因此，需要把集群限流和单机限流配合起来使用，特别是下游服务的实例数、服务能力等和当前系统有较大差距的时候，集群限流还是必要的。

**一种方案**：是通过收集服务节点的请求日志，统计请求量，并通过限流配置，控制节点限流逻辑：

我将其称为后置限流，即收集各个节点的请求量和既定阈值对比，超过则反馈到各个节点，依赖单机限流进行比例限流。

**另一种方案**：是限流总控服务，根据配置生产token，然后各个节点消费token，正常获取token后才能继续业务：

我将其称为前置限流，预先确定分配好可用的token，省去了汇总和反馈的处理机制，相比而言，这种控制方式要相对精准和优雅。

##### 同步转异步

> ***合作方虽然能力有限，但态度很好，加班加点的处理；而我们的客户也很友好，同意多等等***

一个非常经典的例子，就是第三方支付平台的还款业务，用过的同学应该都有体会，一般都是支付完成之后等一会才会收到销账的通知。

这个时延的底层逻辑是什么呢？

一般的，金融机构的服务接口，因为其数据一致性和系统稳定性的要求，性能方面可能不如互联网公司的系统。

那么，当到了月初月末的还款高峰，如果把支持成功用户的销账请求一股脑的都压给机构，后果可想而知。

但是，对于用户来说，整个流程是可以被拆分的，用户侧只要完成支付操作就可以了。至于最终结果，可以允许延后被通知。

因此，基本上，金融网关在处理机构销账都是异步的，即先将各业务的销账请求落地，然后异步的限速轮询待处理的单据，再和机构交互。

其实，不仅仅是在金融领域，只要我们的业务处理速度存在差异，且流程可以被拆分，即可考虑这种架构思路，来缓解系统压力，保障业务可用性。

#### 降级 - 丢车保帅

> ***事发突然，能力有限，我只能紧着几个重要客户服务！***

那么，什么情况需要降级，什么链路可以被降级呢？

当整个业务处于高峰期，或活动脉冲期，当服务的负载很高，逼近了服务承载阈值，即可以考虑服务降级来保障主功能可用。

可以降级的一定是非核心的链路，比如网购场景下的积分抵扣，如果降级积分抵扣链路，其实不影响大部分的支付功能。

那么，在系统中我们一般采用的降级方案有哪些呢？

**1、页面降级**：即从用户操作页面进行操作，直接限制和截断某功能的入口。

**2、存储降级**：使用缓存方式来降级频繁操作的存储

对于秒杀业务这种写多读少的场景，对DB的压力是非常大的，一般的，我们会采用缓存操作代替DB操作，用异步MQ代替同步接口，也属于一种存储的降级行为。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184532618.png" alt="image-20250614184532618" style="zoom:40%;" />

**3、读降级**：对于非核心信息的读请求禁用

微信的抢红包场景，红包列表的展示属于抢红包的非核心链路，因此，对于列表展示，在业务压力较大的情况下，对头像等信息的读，可以直接禁用。

**4、写降级**：直接禁止相关写操作的服务请求

> ***总结，一句话概括降级的核心--丢车保帅。以损失部分体验的代价，来换取整个业务链路的稳定性和持续可用。***

#### 熔断- 大局观

> ***合作方遇到困难了，不能为了自己把人家逼上绝路，别把自己也拖垮！出于人道主义，还得时不时问询下，Are you ok ？***

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184549773.png" alt="image-20250614184549773" style="zoom:50%;" />

在分布式的环境下，异常是常态。如上图所示，当服务C出现调用异常时，会在服务B中出现大量的请求超时和调用延迟。

这些调用也是需要占用系统资源的，当大量请求积压，服务B的线程池等资源也会随之耗尽，最终导致整个服务链路的雪崩都是有可能的。

因此，当服务C出现异常时，对服务C的调用适当暂停，同时不断监测其接口是否恢复，对于整个链路的健康非常有必要的，上述针对C的处理过程就是熔断。

熔断操作的三个关键点：

- 熔断算法，即什么情况即会被判定为需要熔断
- 熔断后处理，即当前系统不进行远程调用，但调用结果需要有替代逻辑
- 熔断恢复，适当的检测机制，用于结束熔断，恢复正常服务调用。

## Redis 集群整个都挂了，该如何处理

### 背景

- 面试官：“如果你项目中的 Redis 集群整个都挂了，这时你要如何处理应对呢？”

候选人：“这个没关系的，因为 Redis 有 RDB 和 AOF 两种持久化方式，在 Redis 4.0 以后还支持 RDB + AOF 的混合持久化，所以挂了也没事。等 Redis 集群重启之后，数据是可以很快恢复过来的。”

- 面试官解释道：“我的意思是，如果 Redis 集群挂了，那系统中所有请求就都落在数据库上了，数据库应该也很快会被打挂吧。而一旦数据库挂了，那系统就整体不可用了。”

- 面试官：“另外，就算把 Redis 集群中的数据恢复了，大概率也不能用了。因为在 Redis 集群挂了的这个时间段，仍然会有很多数据修改或删除的请求打过来，Redis 集群中的数据并没有随之进行更改或删除，那就会出现脏数据的情况。”

候选人想了十秒钟，说道：“嗯嗯，是的，我确实没考虑到这个问题。”

> 面试官希望听到的是，当 Redis 集群挂了的时候，候选人如何从保障系统可用性的角度，给出范围可控且快速止损的解决方案，而绝不仅仅是给出一个“Redis 持久化”相关知识的八股文。

### 1. 启动限流降级

当“Redis 集群挂了”的情况下，我们需要迅速启用限流或降级策略，把系统接收请求的数量和质量，控制在其所能承载处理的范围之内。说白了就是，让失去 Redis 集群支撑的业务系统，不要被外部流量直接击垮。

#### 限流

限流的目的，旨在保护系统不被超出其处理能力的请求冲垮，通过拒绝请求的方式，保证系统的可用性。

限流的关键点在于其限流阈值设置上。

探究系统合理阈值的方式，还是在系统的业务低峰期，以真实流量回放、并递增加压的方式（1 倍、1.5 倍、2 倍、2.5 倍、3 倍等）进行压测，探查系统所能承载的最大容量，然后将限流阈值设置为其峰值容量的 50%～70%。

因此，如果我们所负责的系统重要、等级足够高的话，是需要专门压测“Redis 集群挂了”这个场景的，以此得出一个合理的压测阈值出来。

#### 降级

服务降级，是指当系统出现高负载或异常时，通过牺牲部分非核心功能的方式，保证系统核心功能的可用性。这是一种“弃车保帅”的策略。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184606778.png" alt="image-20250614184606778" style="zoom:50%;" />

### 2. 空集群预热

在“Redis 集群挂了”这个时间段，仍然会有很多数据修改的请求打过来，Redis 集群中的数据并没有随之更改或删除，那就会出现脏数据的情况。

这种情况下，我们需要将一个空的 Redis 集群，投入到生产环境的使用中，通过将用户实时请求的数据吃进缓存方式将其慢慢预热。

切记，此时不要把限流或降级一下子放开，因为当前形同虚设的空 Redis 集群，还不能起到保护数据库的作用，数据库容易被瞬间激增的流量打挂。

### 3. 逐步放开限流降级

随着 Redis 集群中缓存的数据越来越多，可以将限流或降级逐步放开，直至恢复到正常状态。

下面进行举例说明：

（1）在正常状态下，假设我们的系统将限流阈值设置为 5000，当 Redis 集群挂了的时候，我们将系统的限流阈值调整到了 1500。

当 Redis 新集群中的数据量恢复到正常状态下 20% 的时候，我们可以将系统的限流阈值调整到 2000；恢复到 40% 的时候，将系统的限流阈值调整到 2500……以此类推。

（2）我们系统中的功能等级，按照其重要程度分为 P0、P1、P2、P3 四个等级。当 Redis 集群挂了的时候，我们将系统降级到只支持 P0 等级的功能。

当 Redis 新集群中的数据量恢复到正常状态下 30% 的时候，我们可以将系统的降级策略调整到支持 P0 和 P1 等级；恢复到 60% 的时候，将系统的降级策略恢复到支持 P0、P1 和 P2 等级……以此类推。

另外，此时我们也要重点关注数据库服务器中的各种硬件指标，如：系统 Load、IOPS、CPU 利用率等等。

## 如何设计一个秒杀系统

### 热点数据处理

- 热点数据指的就是某一时间段内被大量访问的数据，比如爆款商品的数据
- 预热：定时任务(elastic-job) 统计热点 key，refresh redis 缓存中

### 流量削峰

- 秒杀开始后，流量大，将请求放进消息队列中，消费 mq 慢慢处理

### 集群化

- redis 集群部署

### 限流

- Sentinel

### 降级

- 当请求量达到一个阈值的时候，我们对系统中一些非核心的功能直接关闭或者让它们功能降低

### 熔断

- 如果服务 A 上的商品管理接口响应非常慢的话，其他服务直接不再请求服务 A 上的商品管理这个接口，从而有效避免其他服务被拖慢甚至拖死

### 一致性

- 下单即减库存

    - 提前将秒杀商品的信息放到缓存中去
    - Redis 对库存进行原子操作 lua
    - ![image-20250614184630225](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184630225.png)

### 接口幂等

参考 分布式幂等

### 风控

- 对用户的提交时间做校验，比如提交时间过短（<1s）

## 你所负责的系统的 QPS（TPS）提升十倍或百倍，你要做哪些事情来应对

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184639626.png" alt="image-20250614184639626" style="zoom:50%;" />

### 1. 压力测试

其实道理很简单，既然系统的 QPS（TPS） 提升十倍或百倍了，那我们首先要做的事情，就是通过压力测试的方式去构建 QPS（TPS） 提升十倍或百倍的业务场景。

我认为最好的方案是，在系统的业务低峰期以真实流量回放、并递增加压的方式（1 倍、1.5 倍、2 倍、2.5 倍、3 倍等）在生产环境上进行压测，观察系统的瓶颈点在哪里。

该瓶颈点可能出在数据库服务器的 CPU 利用率、磁盘 IOPS、网络 IO 上，也可能出在数据库中的锁征用上，或是把应用服务器的 Tomcat / Jetty 的线程池打满了，又或是消息队列中的消息积压了，等等。

### 2. 代码优化

当我们找出系统中的瓶颈点后，就可以顺藤摸瓜地跟进到具体的业务代码实现上。

**示例一：**

应用服务器的 Tomcat / Jetty 的线程池打满了，用户请求已经开始积压阻塞的时候，我们可以通过 Skywalking 链路追踪工具进行查看，是否出现系统中某接口，以及某接口中的某个链路步骤（如：下游系统调用、数据库操作、接口自身业务逻辑）出现了耗时较多的情况。

定位到该问题的具体实现代码或 SQL 语句后，则开始进行性能优化。

**示例二：**

当发现数据库服务器的 CPU 利用率达到了 100% 时，我们可以查询是否有特别消耗数据库硬件资源的慢查 SQL，若有的话将其摘取出来，通过 Explain 关键字来查看该慢查 SQL 所对应的执行计划，分析过后通过添加索引、重写 SQL、FORCE INDEX 干涉等方式进行优化。

**示例三：**

当发现消息队列的消息积压严重，消费者的处理速度跟不上生产者的创建速度时，可以通过 SQL 语句单次改批量、逻辑校验串行改并行、高频访问的数据预热、代码逻辑简化及复用等方式，来提升消费者的消息处理吞吐量。

代码优化过后，我们重新回到步骤 1 ，继续进行系统的压力测试工作，看看系统现在是否能承载的峰值 QPS （TPS）达到目标值 1.3 倍以上。

> btw：之所以需要达到目标值的 1.3 倍以上，主要是为了让系统留有余量，这样能够容纳一些预估偏差和数据增量带来的影响。

### 3. 硬件扩容

当我们把系统中存在问题的代码全部优化完毕，继续进行压测却依然达不到目标 QPS 时 ，就可以考虑通过硬件扩容的方式来进行解决了，因为此时我们已经别无他法了。

当然，硬件扩容有分为两种：`纯硬件扩容`​和`技术方案改造 + 硬件扩容`​。

**纯硬件扩容**

举个例子来说，随着业务的迅猛发展，我们的数据库服务器已经不能扛住系统中所有的 SQL 操作时，我们可以创建两个数据库从库，并将对时延性要求较低的业务场景的对应 SQL 迁移过来，以此分担数据库主库的压力。

**技术方案改造 + 硬件扩容**

最贴切的例子就是，我们的数据库主库中的业务主表进行分库分表，以此解决 MySQL 数据库在海量数据的存储和并发读写瓶颈问题。这时，我们需要在业务代码中来解决分库分表方案所带来的各种问题（数据时延性、分库分表路由、结果数据聚合等）。

硬件扩容完成后，我们重新回到步骤 1 ，继续进行系统的压力测试工作，探测本次的扩容操作是否到位。若仍然无法支撑目标 QPS 或 TPS，则继续进行扩容，直到系统所能承载的峰值 QPS （TPS）达到目标值 1.3 倍以上为止。

### 4. 限流兜底

这是最后一步，也是系统的最终`兜底策略`​，如果系统之前没有引入限流策略的话，这次一定要加上。

我们虽然一直在生产环境上进行压测，但当真正十倍或百倍级 QPS（TPS）到来之前，谁也没有真正的把握一定能扛住这波流量。

我在前文中写过，限流的关键点，从来不在工具上，而在于其阈值设置和多级布控上，我们这里着重讲讲阈值设置。

最终的系统限流阈值设定，应该是大于等于目标的 QPS（TPS），并小于系统所能承载的峰值 QPS（TPS）。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184656098.png" alt="image-20250614184656098" style="zoom:50%;" />

## 高并发优惠券系统

### 技术选型

#### 需求拆解

- 要配置券，需要券模版
- 要发券，需要券记录

#### 存储

- 券模板、券记录都需要持久化，而且需要支持条件查询，所以选用 mysql

#### 缓存

- 发券时需要券模板信息，大流量，不可能每次查 mysql，引入 redis 缓存
- 券库存管理，高频操作，也引入 redis 缓存

#### 消息队列

- 券模板/券记录都需要展示过期状态，并且根据不同的状态进行业务逻辑处理，因此有必要引入延迟消息队列来对券模板/券状态进行处理

### 核心逻辑

#### 发券

发券流程分为三部分：参数校验、幂等校验、库存扣减

- 参数校验

    - 用户 id 不为空，黑名单用户，订单号不为空 ==
- 幂等校验

    - 保证发券请求不正确的情况下，业务方通过重试、补偿的方式再次请求，可以最终只发出一张券
- 业务处理

    - redis 获取券详情信息
    - 判断券是否能领取
- 库存扣减

    - redis 扣减库存，异步刷 db

#### 券过期

- 发送延时消息，更新券过期状态

#### 查询券（高频）

- Redis 获取券模板失败时，内部重试
- 引入本地缓存（二级缓存）

## 设计一个抢红包系统

### **需求分析**

> 有一亿的金额，发出来10亿个红包，TPS 预计在百万级，要怎么设计

### 系统设计目标

- 高性能

    - 为了保证用户体验，用户可以尽快看到结果，尽快把抢到的金额加到账户
- 高可靠

    - 不能超发，红包超发或者促销活动超卖，都会给到企业带来损失
- 高可用

    - 活动期间保证服务不挂

### 如何实现高性能

#### 商业级负载均衡器

可以考虑用商业级负载均衡器，F5

#### 减少网络延迟（预分配）

不选商业级负载均衡器，也可以选择预分配

- 直接把红包，根据规则分好，放到不同机房；
- 不同地区的用户，在活动开始前，就已经分配好了机房
- 可以用http DNS或者不同域名，实现机房流量调度
- 当用户抢红包或者查询红包时， 只需要在本地机房处理即可，然后把结果异步发送mq，同步账户服务

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184714403.png" alt="image-20250614184714403" style="zoom:30%;" />

##### 后端服务设计

通过机房流量调度，可以把并发压力降下来，但是每个机房还是有几十万的TPS，那该怎么办？

继续分而治之。

- 存储到redis，直接部署多个不同实例，假设要达到50w的TPS，按照单机8w的TPS来计算，只需要7个实例，可以冗余部署10个实例；
- 应用层做轮训，如果应用层发现实例挂了，马上剔除掉
- 每个redis实例，都会存储一个拆分好的红包id+红包金额 list
- 抢红包的时候，用lua脚步，从list里面pop一个元素，同时记录到另一个list里面，存储uid + 红包id + 红包金额
- 定时任务集群，不断从redis集群中取uid+金额数据，批量插入mysql集群，同时发送mq，通知账户服务
- mysql集群用uid为维度分库，主要用来查询用户已抢到的红包列表

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184731842.png" alt="image-20250614184731842" style="zoom:20%;" />

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184747081.png" alt="image-20250614184747081" style="zoom:20%;" />

简单来说，就是通过分而治之的一些方法，能够把流量分散到不同地方，达到高性能效果。

### 如何实现高可靠

#### 红包拆分算法

##### 二倍均值法

把每次随机金额的上限定为剩余人均金额的2倍。

> - 红包金额为 M 元
> - 剩余人数为 N
> - 每次抢到的金额 = 随机区间 (0，M / N × 2)元

保证了**每一次抢到金额随机范围的均值是相等的**，不会因为[抢红包](https://so.csdn.net/so/search?q=%E6%8A%A2%E7%BA%A2%E5%8C%85&spm=1001.2101.3001.7020)的先后顺序而造成不公平。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184804738.png" alt="image-20250614184804738" style="zoom:30%;" />

```java
  /**
     * 分配红包的金额
     *
     * @param totalAmount    总金额（以分为单位）
     * @param totalPeopleNum 总人数
     * @return
     */
    public static List<Integer> divideRedPackage(Integer totalAmount, Integer totalPeopleNum) {
        List<Integer> amountList = new ArrayList<Integer>();//存储分配的不同红包大小的数值
        Integer restAmount = totalAmount;
        Integer restpeopleNum = totalPeopleNum;
        Random random = new Random();
        //最后一个人不用随机分配了，之间差值
        for (int i = 0; i < totalPeopleNum - 1; i++) {
            // 不能从0开始随机，我们还得保证每个人分的钱至少是0.01
            // 随机范围：[1, 剩余人均金额的两倍)
            int amount = random.nextInt((restAmount / restpeopleNum) * 2 - 1) + 1;
            restAmount = restAmount - amount;
            restpeopleNum--;
            amountList.add(amount);
        }
        amountList.add(restAmount);
        return amountList;
    }

    public static void main(String[] args) {
        List<Integer> amountList = divideRedPackage(5000, 30);
        for (Integer amount : amountList) {
            System.out.println("抢到" + new BigDecimal(amount).divide(new BigDecimal(100)) + "元");
        }
    }
```

> 缺点：不会给用户太多惊喜，红包金额上限不高（除了最后一个红包）
>
> 优点：实现简单，空间复杂度低

##### 线段切割法

把红包总金额想象成一条很长的线段，而每个人抢到的金额，则是这条主线段所拆分出的若干子线段。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184821655.png" alt="image-20250614184821655" style="zoom:15%;" />

确定每一条子线段的长度  
由“切割点”来决定。  
当n个人一起抢红包时，就需要确定n-1个切割点，也就是做n-1次随机运算

当所有切割点确定以后，子线段的长度也随之确定。此时红包的拆分金额，就等同于每个子线段的长度。

```java
/**
     * 拆分红包V2
     * @param totalAmount  总金额（以分为单位）
     * @param totalPeopleNum  总人数
     */
    public static List<Integer> divideRedPackageV2(Integer totalAmount, Integer totalPeopleNum){
        List<Integer> amountList = new ArrayList<Integer>();
        Set<Integer> segments = new HashSet<Integer>();
        Random random = new Random();
        while(segments.size() <= totalPeopleNum) {
            // 切割的位置segment,随机的范围区间是[1， m-1]
            int segment =  random.nextInt(totalAmount-1) + 1;
            // 这是异常的情况
            // segments.contains(segment) ==true 表示切割位置重复
            //segment == 0,切割位置不正确
            if(segments.contains(segment) || segment == 0) {
                continue;
            }
            segments.add(segment);
        }

        //依据切割的段来划分 钱数
        //从HashSet取值
        //Collections.sort排序方法，把切割位置从小到大排序。
        List<Integer> segmentList = new ArrayList<Integer>(segments);
        Collections.sort(segmentList);
        for(int i=0; i<segmentList.size(); i++){
            Integer amount;
            if(i==0){
                amount = segmentList.get(0);
            }else {
                amount = segmentList.get(i) - segmentList.get(i-1) ;
            }
            amountList.add(amount);
        }
        amountList.add(totalAmount - segmentList.get(segmentList.size()-1));

        return amountList;
    }
```

> 缺点：实现不简单
>
> 优点：相对来说会给用户惊喜

#### 风控防刷

每年都会有大量的囤积账号，准备发财，风控规则很复杂，但是风控和高可用是矛盾的，有所取舍。

### 如何实现高可用

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184846163.png" alt="image-20250614184846163" style="zoom:25%;" />

### **表结构设计**

红包活动表：

```sql
CREATE TABLE `t_redpack_activity`
(
    `id`         bigint(20)     NOT NULL COMMENT '主键',
    `total_amount`     decimal(10, 2) NOT NULL DEFAULT '0.00' COMMENT '总金额',
    `surplus_amount`     decimal(10, 2) NOT NULL DEFAULT '0.00' COMMENT '剩余金额',
    `total` bigint(20)     NOT NULL DEFAULT '0' COMMENT '红包总数',
    `surplus_total` bigint(20)     NOT NULL DEFAULT '0' COMMENT '红包剩余总数',
    `user_id`    bigint(20)     NOT NULL DEFAULT '0' COMMENT '用户编号',
    `version` bigint(20)     NOT NULL DEFAULT '0' COMMENT '版本号',
    PRIMARY KEY (`id`)
) ENGINE = InnoDB
DEFAULT CHARSET = utf8;
```

红包表：

```sql
CREATE TABLE `t_redpack`
(
    `id`         bigint(20)     NOT NULL COMMENT '主键',
    `activity_id`         bigint(20)     NOT NULL DEFAULT 0 COMMENT '红包活动ID',
    `amount`     decimal(10, 2) NOT NULL DEFAULT '0.00' COMMENT '金额',
    `status`     TINYINT(4) NOT NULL DEFAULT 0 COMMENT '红包状态 1可用 2不可用',
    `version` bigint(20)     NOT NULL DEFAULT '0' COMMENT '版本号',
    PRIMARY KEY (`id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8;
```

明细表：

```sql
CREATE TABLE `t_redpack_detail`
(
    `id`         bigint(20)     NOT NULL COMMENT '主键',
    `amount`     decimal(10, 2) NOT NULL DEFAULT '0.00' COMMENT '金额',
    `user_id`    bigint(20)     NOT NULL DEFAULT '0' COMMENT '用户编号',
    `redpack_id` bigint(20)     NOT NULL DEFAULT '0' COMMENT '红包编号',
    `create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间',
    `update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '更新时间',
    PRIMARY KEY (`id`)
) ENGINE = InnoDB
  DEFAULT CHARSET = utf8;
```

- 活动表，就是你发了多少个红包，并且需要维护剩余金额。
- 明细表，是用户抢到的红包明细。
- 红包表，是每一个具体的红包信息。

## 电商系统架构常见大坑

### 一、避免重复下单

用户快速点了两次 “提交订单”  按钮，浏览器会向后端发送两条创建订单的请求，最终会创建两条一模一样的订单。

**解决方案：**

解决方案就是采用**幂等机制**，多次请求和一次请求产生的效果是一样的。

参考 分布式幂等

### 二、库存超卖

常见的库存扣减方式有：

- 下单减库存：即当买家下单后，在商品的总库存中减去买家购买数量。下单减库存是最简单的减库存方式，也是控制最精确的一种，下单时直接通过数据库的事务机制控制商品库存，这样一定不会出现超卖的情况。但是你要知道，有些人下完单可能并不会付款。
- 付款减库存：即买家下单后，并不立即减库存，而是等到有用户付款后才真正减库存，否则库存一直保留给其他买家。但因为付款时才减库存，如果并发比较高，有可能出现买家下单后付不了款的情况，因为可能商品已经被其他人买走了。
- 预扣库存：这种方式相对复杂一些，买家下单后，库存为其保留一定的时间（如 30 分钟），超过这个时间，库存将会自动释放，释放后其他买家就可以继续购买。在买家付款前，系统会校验该订单的库存是否还有保留：如果没有保留，则再次尝试预扣；如果库存不足（也就是预扣失败）则不允许继续付款；如果预扣成功，则完成付款并实际地减去库存。

至于采用哪一种减库存方式更多是业务层面的考虑，减库存最核心的是大并发请求时保证数据库中的库存字段值不能为负数。

**方案一：**

通常在扣减库存的场景下使用行级锁，通过数据库引擎本身对记录加锁的控制，保证数据库的更新的安全性，并且通过`where`​语句的条件，保证库存不会被减到 `0`​ 以下，也就是能够有效的控制超卖的场景。

```sql
update ... set amount = amount - 1 where id = $id and amount - 1 >=0
```

**方案二：**

设置数据库的字段数据为无符号整数，这样减后库存字段值小于零时 SQL 语句会报错。

### 商家发货，物流单更新 ABA 问题

举个例子：

商家发货，填写运单号，开始填了 123，后来发现填错了，然后又修改为 456。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184910803.png" alt="image-20250614184910803" style="zoom:80%;" />

过程：

- 开始「请求A」发货，调订单服务接口，更新运单号 `123`​
- 但是响应有点慢，超时了
- 此时，商家发现运单号填错了，发起了「请求B」，更新运单号为 `456`​ ，订单服务也响应成功了
- 这时，「请求A」触发了重试，再次调用订单服务，更新运单号 `123`​，订单服务也响应成功了
- 订单服务最后保存的 运单号 是 `123`​

**理想的解决方案：**

数据库表引入一个额外字段  `version`​，每次更新时，判断表中的版本号与请求参数携带的版本号是否一致

```sql
update order
set logistics_num = #{logistics_num} , version = #{version} + 1
where order_id= 1111 and version = #{version}
```

- 一致：才触发更新
- 不一致：说明这期间执行过数据更新，可能会引发错误，拒绝执行。

### 历史订单，归档

根据**二八定律**，系统绝大部分的性能开销花在20%的业务。数据也不例外，从数据的使用频率来看，经常被业务访问的数据称为热点数据；反之，称之为冷数据。

在了解的数据的冷、热特性后，便可以指导我们做一些有针对性的性能优化。这里面有业务层面的优化，也有技术层面的优化。比如：电商网站，一般只能查询3个月内的订单，如果你想看看3个月前的订单，需要访问历史订单页面。

**实现思路：**

1、冷热数据区分的标准是什么？要结合业务思考，可能要找产品同学一块讨论才能做决策，切记不要拍脑袋。以电商订单为例：

- 方案一：以“下单时间”为标准，将3 个月前的订单数据当作冷数据，3 个月内的当作热数据。
- 方案二：根据“订单状态”字段来区分，已完结的订单当作冷数据，未完结的订单当作热数据。
- 方案三：组合方式，把下单时间 > 3 个月且状态为“已完结”的订单标识为冷数据，其他的当作热数据。

2、如何触发冷热数据的分离

- 方案一：直接修改业务代码，每次业务请求触发冷热数据判断，根据结果路由到对应的冷数据表或热数据表。缺点：如果判断标准是 `时间维度`​，数据过期了无法主动感知。
- 方案二：如果觉得修改业务代码，耦合性高，不易于后期维护。可以通过监听数据库变更日志 binlog 方式来触发
- 方案三：常用的手段是跑定时任务，一般是选择凌晨系统压力小的时候，通过跑批任务，将满足条件的冷数据迁移到其他存储介质。在途业务表中只留下来少量的热点数据。

3、如何实现冷热数据分离，过程大概分为三步：

- 判断数据是冷、还是热
- 将冷数据插入冷数据表中
- 然后，从原来的热库中删除迁移的数据

4、如何使用冷热数据

- 方案一：界面设计时会有选项区分，如上面举例的电商订单
- 方案二：直接在业务代码里区分。

### 订单分库分表，多维度查询

如果电商网站的订单数过多，我们一般会想到 `分库分表`​ 解决策略。没问题，这个方向是对的。

**但是查询维度很多**

1、买家，查询 `我的订单`​ 列表，需要根据 `buyer_id`​ 来查询

2、查看订单详情，需要根据 `order_id`​ 来查询

3、卖家，查询 `我的销售`​ 列表，需要根据 `seller_id`​ 来查询

而订单分表只有一个分表键，如何满足多维度 SQL 操作呢？

我们一般是基于买家维度来设计，下图是 `淘宝`​ 的订单列表

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184929857.png" alt="image-20250614184929857" style="zoom:40%;" />

一个订单号 19 位，我们会发现同一个用户不同订单的最后 6 位都是一样的，没错，那是用户id的后6位。

这样，上文中  `场景1`​、`场景2`​ 的查询可以共性抽取， 采用 `buyer_id`​ 或 `order_id`​  的 `后六位`​ 作为分表键，对 `1 000 000`​ 取模，得到买家维度的订单分表的编号。

至于 `场景3`​ 卖家维度的订单查询，我们可以采用数据异构方式，按 `seller_id`​ 维度另外存储一份数据，专门供卖家使用！！！

## 异步请求如何转同步

### 背景

现有一个系统，整体架构如下所示:

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614184948027.png" alt="image-20250614184948027" style="zoom:20%;" />

这是一个很常见的同步设计方案，上游系统需要等待下游系统接口返回调用结果。

现在需要接入另外一个第三方服务 B，该服务与服务 A 最大区别在于，这是一个**异步** `API`​。调用之后，仅仅返回**受理成功**，处理结果后续通过异步通知返回。

接入之后，整体架构如下所示：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185002411.png" alt="image-20250614185002411" style="zoom:30%;" />

> 由于网络隔离策略，通知接收程序与通信服务需要单独分开部署。若没此要求，可以将通信服务 B 与通知接收程序合并成一个应用。
>
> 另外图中所有应用采用双节点部署。

为了不影响 `OpenAPI`​ 上游系统同步处理逻辑，通信服务 B 调用第三方服务之后，不能立刻返回，需要等待结果通知，拿到具体返回结果。这就需要通信服务 B 内部将异步转为同步。

这就是一个典型的异步转同步问题，整个过程涉及两个问题。

1. 通信服务 B 业务线程如何进入**等待**状态？又如何**唤醒**正确等待线程？
2. 由于通信服务 B 双节点部署，通知接收程序如何将结果转发到正在等待处理的节点？

问题 1 的解决方案参考了 Dubbo 设计思路。

我们在使用 Dubbo 调用远程服务时，默认情况下，这是一种阻塞式调用方式，即 Consumer 端代码一直阻塞等待，直到 Provider 端返回为止。

由于 Dubbo 底层基于 `Netty`​ 发送网络请求，这其是一个异步的过程。为了让业务线程能同步等待，这个过程就需要将异步转为同步。

### Dubbo 异步转同步解决办法

#### 2.6.X 版本

##### 业务线程同步阻塞

Dubbo 发起远程调用代码位于 `DubboInvoker#doInvoke`​：

> Dubbo 版本为：2.6.X 版本。2,7.X 重构 `DefaultFuture`​ ，但是本质原理还是一样。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185019305.png" alt="image-20250614185019305" style="zoom:30%;" />

默认情况下，Dubbo 支持同步调用方式，这里将会创建 `DefaultFuture`​ 对象。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185035601.png" alt="image-20250614185035601" style="zoom:20%;" />

这里有个非常重要逻辑，每个请求生成一个唯一 **ID**，然后将 `ID`​ 与 `DefaultFuture`​ 映射关系，存入 `Map`​ 中。

这个请求 **ID** 在之所以这么重要，是因为消费者并发调用服务发送请求，同时将会有多个业务线程进入阻塞。当收到响应之后，我们需要唤醒正确的等待线程，并将处理结果返回。

通过 **ID** 这个唯一映射的关系，很自然可以找到其对应 `DefaultFuture`​，唤醒其对应的业务线程。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185258559.png" alt="image-20250614185258559" style="zoom:30%;" />

业务线程调用 `DefaultFuture#get`​方法进入阻塞。这段代码比较简单，通过调用 `Condition#await`​ 阻塞线层。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185319677.png" alt="image-20250614185319677" style="zoom:35%;" />

##### 唤醒业务线程

当消费者接收到服务提供者的返回结果，将会调用 `DefaultFuture#received`​ 方法。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185344108.png" alt="image-20250614185344108" style="zoom:30%;" />

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185359843.png" alt="image-20250614185359843" style="zoom:38%;" />

通过响应对象中的唯一 **ID**，找到其对应 `DefaultFuture`​ 对象，从而将结果设置 `DefaultFuture`​ 对象中，然后唤醒的相应的业务线程。

#### 2.7.X 版本

##### 业务线程同步阻塞

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185431345.png" alt="image-20250614185431345" style="zoom:40%;" />

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185445777.png" alt="image-20250614185445777" style="zoom:45%;" />

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185501471.png" alt="image-20250614185501471" style="zoom:50%;" />

##### 唤醒业务线程

当消费者接收到服务提供者的返回结果，将会调用 `DefaultFuture#received`​ 方法。

完成任务的时候，会走到 `postComplete`​ 方法：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185518691.png" alt="image-20250614185518691" style="zoom:60%;" />

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185536970.png" alt="image-20250614185536970" style="zoom:50%;" />

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185548529.png" alt="image-20250614185548529" style="zoom:55%;" />

#### 主要思路汇总

1. **Future 机制**：

    - Dubbo 允许你为每个异步调用返回一个 `Future`​ 对象。这个 `Future`​ 对象可以用来同步等待异步操作的结果。
    - 当调用 Dubbo 服务时，你可以获取一个 `Future`​ 对象，然后使用 `Future`​ 的 `get()`​ 方法来等待结果，这个方法会阻塞直到异步操作完成。
2. **CompletableFuture 集成**：

    - Dubbo 还提供了对 Java 8 引入的 `CompletableFuture`​ 的集成。`CompletableFuture`​ 提供了更强大的异步编程能力，包括链式调用、组合多个异步操作等。
    - Dubbo 允许你使用 `CompletableFuture`​ 来包装异步调用，从而能够使用 `CompletableFuture`​ 的链式调用和回调功能。

### 转发方案设计

根据 Dubbo 解决思路，问题 1 解决办法就比较简单了。具体流程如下：

1. 通信服务 B 内部生成一个唯一请求 **ID** ，发给第三方服务
2. 若请求成功，内部版使用 `Map`​ 存储对应关系，并使业务线程阻塞等待
3. 通信服务 B 收到异步通知结果，通过 **ID** 查找对应业务线程，唤醒的相应的线程

这个设计过程需要注意设置合理的**超时时间**，这个超时时间需要考虑远程服务调用耗时，可以参考如下公式：

```armasm
业务线程等待时间=通信服务 B 接口的超时时间 - 调用第三方服务 B 接口消耗时间
```

这里就不贴出具体的代码，详细代码参考 Dubbo `DefaultFuture`​。

接下来重点看下通知服务如何将结果转发给正确的通信服务 B 的节点。

MQ 方案相对简单，这里采用 MQ 广播消费的方式，架构如图所示：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250614185612107.png" alt="image-20250614185612107" style="zoom:30%;" />

通知接收程序收到异步通知之后，直接将结果发送到 `MQ`​。

通信服务 B 开启广播消费模式，拉取 `MQ`​ 消息。

通信服务 B_1 拉取消息，通过请求 **ID** 映射关系，没找到内部等待的线程，知道这不是自己的等待消息，于是 B_1 直接丢弃即可。

通信服务 B_2 拉取消息，通过请求 **ID** 映射关系，顺利找到正在等待的线程，然后可以唤醒等待线程，返回最后的结果。

对比 `SocketServer`​ 方案，`MQ`​ 方案整体流程比较简单，编程难度低，也没用存在特殊的配置。

不过这个方案十分依赖 `MQ`​ 消息实时性，若 `MQ`​ 消息投递延迟很高，这就会导致通信服务 B 业务线程超时苏醒，业务异常返回。

这里我们选择使用 `RocketMQ`​，长轮询 `Pull`​ 方式，可保证消息非常实时，

综上，这里采用 `MQ`​ 的方案。

## 单机系统演变为分布式系统，会涉及到哪些技术的调整？请从前面负载到后端详细描述下？

单机系统演变为分布式系统是一个复杂的过程，涉及多个层面的技术调整。以下是从前端负载到后端的技术调整详细描述：

### 前端负载

1. **负载均衡**：

    - 引入负载均衡器（如Nginx, HAProxy等）以分发用户请求到不同的服务器。
    - 实现健康检查机制，确保请求只被分发到健康的后端服务器。
2. **CDN（内容分发网络）** ：

    - 使用CDN可以将静态资源分发到全球多个节点，减少用户访问延迟。
3. **DNS解析**：

    - 使用智能DNS解析，根据用户地理位置或网络状况将请求导向最近或最佳的服务节点。

### 后端服务

1. **服务拆分**：

    - 单体应用拆分为微服务架构，每个服务负责一部分功能。
    - 使用服务发现机制（如Zookeeper, Consul, Eureka等）来管理服务实例的注册与发现。
2. **API网关**：

    - 引入API网关作为所有客户端请求的单一入口点，处理路由、认证、限流、熔断等。
3. **数据一致性与分布式事务**：

    - 采用分布式数据库和缓存解决方案，如分布式关系数据库、NoSQL数据库、Redis集群等。
    - 实现分布式事务管理，如使用两阶段提交（2PC）、事务补偿、最终一致性等策略。
4. **消息队列**：

    - 使用消息队列（如Kafka, RabbitMQ, RocketMQ等）进行服务间的异步通信，提升系统解耦和扩展性。

### 存储层

1. **数据库分片**：

    - 实现数据库分片（Sharding）以支持数据水平扩展。
2. **数据复制与备份**：

    - 实施数据复制机制（如主从复制、多主复制）以保证数据的高可用性和灾难恢复。
3. **分布式文件系统**：

    - 使用分布式文件系统（如HDFS, Ceph等）来存储大量文件数据。

### 网络层

1. **服务间通信**：

    - 使用RPC框架（如gRPC, Thrift等）进行服务间的高效通信。
    - 确保网络通信安全，使用TLS/SSL等加密手段。
2. **虚拟网络**：

    - 在云环境中使用虚拟网络（如VPC）来隔离不同服务之间的网络。

### 监控与运维

1. **日志收集与分析**：

    - 实施集中式日志管理系统（如ELK Stack, Fluentd等）。
2. **监控与告警**：

    - 使用监控工具（如Prometheus, Grafana等）来监控系统的健康状况。
    - 设定告警机制，及时发现并响应系统异常。
3. **自动化运维**：

    - 引入自动化部署工具（如Kubernetes, Ansible等）来管理服务部署。
    - 实施自动化测试和持续集成/持续部署（CI/CD）流程。

### 安全性

1. **认证授权**：

    - 实现分布式认证授权机制，如OAuth 2.0、JWT等。
2. **网络安全**：

    - 部署防火墙、入侵检测系统（IDS）和入侵预防系统（IPS）。
3. **数据安全**：

    - 对敏感数据进行加密存储和传输。  
      通过这些技术调整，单机系统可以平滑地过渡到分布式系统，从而实现高可用性、可扩展性和高效率。