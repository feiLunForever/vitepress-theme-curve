---
title: 工业界的推荐系统
tags:
  - AI
categories:
  - AI
date: '2025-01-13'
description: 欢迎使用 Curve 主题，这是你的第一篇文章
articleGPT: 这是一篇初始化文章，旨在告诉用户一些使用说明和须知。
#cover: "/images/logo/logo.webp"
---


# 工业界的推荐系统

# 概要

> * [概要01：推荐系统的基本概念.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672040736347-cdf73454-ede6-4edc-9fa2-385991ef85e8.pdf)
> * [概要02：推荐系统的链路.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672040736752-37761210-9f1b-4413-a6c5-39807e742cac.pdf)
> * [概要03：AB测试.pdf](https://www.yuque.com/yuejiangliu/recommended-system-in-the-industry/assets/%E6%A6%82%E8%A6%8103%EF%BC%9AAB%E6%B5%8B%E8%AF%95-20221225101406-cng75lv.pdf)

## 推荐系统的基本概念

![1672040736924-29d2193a-471e-4015-b142-1460bef19b38](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672040736924-29d2193a-471e-4015-b142-1460bef19b38-20240704162514-o9xdz6m.png)

### 消费指标：反映用户对推荐是否满意

* 点击率 = 点击次数 / 曝光次数 —— 越高，证明推荐越精准 —— 不能仅追求它，不然都是标题党了
* 点赞率 = 点赞次数 / 点击次数
* 收藏率 = 收藏次数 / 点击次数
* 转发率 = 转发次数 / 点击次数
* 阅读完成率 = 滑动到底次数 / 点击次数 × f(笔记长度) —— f 用于归一化

这些都是短期消费指标，不能一味追求

* 因为重复推荐相似内容可以提高消费指标，但容易让用户腻歪，进而降低用户活跃度
* 而尝试提供多样性的内容，可以让用户发现自己新的兴趣点，提高活跃度（但在这个过程中，消费指标可能下降）

> 反正就是尽量让用户多在平台投入精力

### 北极星指标：衡量推荐系统好坏

在小红书考虑下面 3 个

* 用户规模：

  * 日活用户数（DAU）、月活用户数（MAU）
  * DAU：每天使用 1 次以上
  * MAU：每月使用 1 次以上

* 消费：

  * 人均使用推荐的时长、人均阅读笔记的数量
* 发布：

  * 发布渗透率、人均发布量

> 北极星指标 都是线上指标，只能上线了才能获得

### 实验流程

![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704165105-2c6zlds.png)

## 推荐系统的链路

![1672040737013-f61fc9a3-6159-42d1-a84c-dfe6a798368a](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672040737013-f61fc9a3-6159-42d1-a84c-dfe6a798368a-20240704162514-tp0eqaq.png)

* 粗排：用到规模比较小的模型
* 精排：用到大规模深度神经网络，对 items 进行打分
* 召回通道：协同过滤、双塔模型、关注的作者、等等​

  * ![1672040737101-86c337d2-1134-4829-b25a-4d0ee6e7a1be](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672040737101-86c337d2-1134-4829-b25a-4d0ee6e7a1be-20240704162514-9cpg68v.png)​
  * 小红书有几十个召回通道，每个通道返回几十上百篇笔记
  * 将所有召回通道的内容融合后，会去重，并过滤（例如去掉用户不喜欢的作者的笔记，不喜欢的话题）
* 排序： [几千] → 粗排 → [几百] → 精排 → [几百] → 重排 → [几十]

  * ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704165705-ub7ucio.png)​
  * 粗排模型小，速度快；精排用的模型大，计算量大，打分更可靠
  * 用粗排做筛选，再用精排 —— 平衡计算量和准确性
  * 本来是可以用精排分数排序后直接推荐的，但此时的结果还存在一些不足（例如 多样性）
  * 重排根据多样性随机抽样，还要用规则将相似的笔记打散，还得把广告插进去

* 粗排、精排

  * ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704165808-3deldoc.png)​

* 重排

  * 做多样性抽样（比如 MMR、DPP），从几百篇中选出几十篇

    * 抽样依据：精排分数、多样性
  * 用规则打散相似笔记

    * 不能把内容过于相似的笔记，排在相邻的位置上

    > 减少一个页面中的同质化内容
    >

    * 插入广告、运营推广内容，根据生态要求调整排序（例如 不能同时出很多美女图片）

**总结**

* 召回：用多条通道，取回几千篇笔记
* 粗排：用小规模神经网络，给几千篇笔记打分，选出分数最高的几百篇

  * 召回和粗排是个大漏斗

* 精排：用大规模神经网络，给几百篇笔记打分
* 重排：做多样性抽样、规则打散、插入广告和运营笔记

## A/B 测试

* A/B 测试举例

  * 召回团队实现了一种 GNN 召回通道，离线实验结果正向
  * 下一步是做线上的小流量 A/B 测试，考察新的召回通道对线上指标的影响
  * 模型中有一些参数，比如 GNN 的深度取值 ∈\{1,2,3\} ，需要用 A/B 测试选取最优参数

* 随机分桶

  * 分 b = 10 个桶，每个桶中有 10% 的用户
  * 首先用哈希函数把用户 ID 映射成某个区间内的整数，然后把这些整数均匀随机分成 b 个桶
  * 全部 n 位用户，分成 b 个桶，每个桶中有  $n/b $ 位用户
  * ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704174333-m1seibn.png)​
  * ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704174627-e2uwgqs.png)​
  * 计算每个桶的业务指标，比如 DAU、人均使用推荐的时长、点击率、等等
  * 如果某个实验组指标显著优于对照组，则说明对应的策略有效，值得推全

### 分层实验

#### 流量不够用怎么办？

* 信息流产品的公司有很多部门和团队，大家都需要做 A/B 测试

  * 推荐系统（召回、粗排、精排、重排）
  * 用户界面
  * 广告
* 如果把用户随机分成 10 组，1 组做对照，9 组做实验，那么只能同时做 9 组实验

#### 分层实验

* 分层实验：召回、粗排、精排、重排、用户界面、广告......（例如 GNN 召回通道属于召回层）
* 同层互斥：GNN 实验占了召回层的 4 个桶，其他召回实验只能用剩余的 6 个桶
* 不同层正交：每一层独立随机对用户做分桶。每一层都可以独立用 100% 的用户做实验

#### 分层实验举例

* 召回层把用户分成 10 个桶：U1,U2,...,U10
* 精排层把用户分成 10 个桶：V1,V2,...,V10
* 设系统共有 n 个用户，那么 $∣U  i   ∣=∣V  j   ∣=n/10$  

  * 即每个召回桶里的用户数与每个精排桶里的用户数是差不多的
* 召回桶 U<sub>i</sub> 和召回桶 U<sub>j</sub> 交集为 $U∩Uj=∅$（空集）

  * 召回桶之间互斥，即 2 个召回实验不会同时作用到相同用户上
* 召回桶 U<sub>i</sub> 和精排桶 V<sub>j</sub> 交集的大小为 $∣U∩V  j   ∣=n/100$

  * 即 1 个用户虽然不能同时受 2 个召回实验的影响，但他可以受 1 个召回实验 + 1 个精排实验的影响
* ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704180015-6clyyod.png)​

#### 不同层正交

![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704180038-evr82pk.png)

* 在用户界面的 2 号桶用户，被随机均匀打撒到了召回的 10 个桶中
* 通常来说 用户界面实验的效果 和 召回实验的效果 不容易相互增强（或抵消），所以允许 1 个用户同时受两层实验的影响

#### 互斥 vs 正交

* 如果所有实验都正交，则可以同时做无数组实验
* 必须使用互斥的原因

  * 同类的策略（例如精排模型的两种结构）天然互斥，对于一个用户，只能用其中一种
  * 同类的策略（例如添加两条召回通道）效果会相互增强（1+1>2）或相互抵消（1+1<2）。互斥可以避免同类策略相互干扰
* 不同类型的策略（例如添加召回通道、优化粗排模型）通常不会相互干扰（1+1=2），可以作为正交的两层

### Holdout 机制

* 每个实验（召回、粗排、精排、重排）独立汇报对业务指标的提升
* 公司考察一个部门（比如推荐系统）在一段时间内对业务指标总体的提升
* 取 10% 的用户作为 holdout 桶，推荐系统使用剩余 90% 的用户做实验，两者互斥
* 10% holdout 桶 vs 90% 实验桶的 diff（需要归一化）为整个部门的业务指标收益

![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704180246-l3ft2s3.png)

* holdout 桶里面不加任何新的实验，保持干净以便对照

![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704180306-a8stj5q.png)

* 每个考核周期结束之后，清除 holdout 桶，让推全实验从 90% 用户扩大到 100% 用户
* 重新随机划分用户，得到 holdout 桶和实验桶，开始下一轮考核周期
* 新的 holdout 桶与实验桶各种业务指标的 diff 接近 0
* 随着召回、粗排、精排、重排实验上线和推全，diff 会逐渐扩大

### 实验推全 & 反转实验

![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704180331-5qeylao.png)

![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704180432-imqcyyk.png)

反转实验

* 有的指标（点击、交互）立刻受到新策略影响，有的指标（留存）有滞后性，需要长期观测
* 实验观测到显著收益后尽快推全新策略。目的是腾出桶供其他实验使用，或需要基于新策略做后续的开发
* 用反转实验解决上述矛盾，既可以尽快推全，也可以长期观测实验指标
* 在推全的新层中开一个旧策略的桶，长期观测实验指标

![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240704180501-k1hnrre.png)

### 总结

* 分层实验：同层互斥（不允许两个实验同时影响一位用户）、不同层正交（实验有重叠的用户）

  * 把容易相互增强（或抵消）的实验放在同一层，让它们的用户互斥
* Holdout：保留 10% 的用户，完全不受实验影响，可以考察整个部门对业务指标的贡献
* 实验推全：新建一个推全层，与其他层正交
* 反转实验：在新的推全层上，保留一个小的反转桶，使用旧策略。长期观测新旧策略的 diff

# 召回

> * [基于物品的协同过滤（ItemCF）.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042311349-5c995842-1db1-4ae3-bb6e-ab5b09f21dc3.pdf)
> * [Swing 召回通道.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042311552-06e66808-8aeb-4b3c-9432-63599b228b62.pdf)
> * [基于用户的协同过滤（UserCF）.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042311689-922f1cbf-1351-482d-bc1a-7ff54cbd4b75.pdf)
> * [离散特征处理.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042311971-9bc3983c-b234-4050-8eb2-d664beb55363.pdf)
> * [矩阵补充.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042312147-e63c67bd-9947-4dfc-b9a6-0b7aa7fb7a60.pdf)
> * [双塔模型：模型和训练.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042312324-21421698-94f7-4b1e-87ad-7a6076c86b02.pdf)
> * [双塔模型：正负样本.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042312529-61989655-13ca-47f7-8565-035dbab25081.pdf)
> * [双塔模型：线上召回和更新.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042312827-66b6b04e-2258-4453-8288-7168e8b5f844.pdf)
> * [其它召回通道.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/101969/1672042313106-8b45b2cc-b4a8-4db7-a904-119a338bfe7a.pdf)

## 基于物品的协同过滤（ItemCF）

### ItemCF 的原理

![1672042313317-423ccc1f-3126-456d-bf78-c1feb6b27061](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042313317-423ccc1f-3126-456d-bf78-c1feb6b27061-20240705151039-u0rdkso.png)

* 推荐系统如何知道《笑傲江湖》与《鹿鼎记》相似？

  * 用知识图谱：两本书的作者相同，所以相似
  * 根据**全体用户的行为**：

    * 看过《笑傲江湖》的用户也看过《鹿鼎记》
    * 给《笑傲江湖》好评的用户也给《鹿鼎记》好评

### ItemCF 的实现

![1672042313418-03a5b3e6-a546-4f8c-a8e3-790a35fe90e5](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042313418-03a5b3e6-a546-4f8c-a8e3-790a35fe90e5-20240705151039-gawxnrf.png "预估用户对候选集物品的兴趣：2×0.1 + 1×0.4 + 4×0.2 + 3×0.6 = 3.2")

### 物品相似度

* 两个物品的 **受众** 重合度越高，两个物品越相似
* 例如：

  * 喜欢《射雕英雄传》和《神雕侠侣》的读者重合度很高
  * 可以认为《射雕英雄传》和《神雕侠侣》相似
  * ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240705152833-wjz1hua.png)​
  * ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240705152845-i1ipq9t.png)​
* 计算物品相似度

  * 喜欢物品 i<sub>1</sub> 的用户记作集合 W<sub>1</sub>
  * 喜欢物品 i<sub>2</sub> 的用户记作集合 W<sub>2</sub>
  * 定义交集 $V=W  1   ∩W  2 $（同时喜欢 i<sub>1</sub> 和 i<sub>2</sub> 的用户）
  * 两个物品的相似度：![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240705151544-sxk07bq.png)​

    * 注： 公式没有考虑喜欢的程度 𝑙𝑖𝑘𝑒(𝑢𝑠𝑒𝑟, 𝑖𝑡𝑒𝑚)
    * 即喜欢就是 1，不喜欢就是 0
* 如果考虑了喜欢程度：

  * ![1672042313526-c9a54e78-1ee5-4adb-a2b7-cb37fb003169](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042313526-c9a54e78-1ee5-4adb-a2b7-cb37fb003169-20240705151040-7f00g1o.png)​
* ItemCF 的基本思想：

  * 如果用户喜欢物品 item<sub>1</sub> ，而且物品 item<sub>1</sub> 与 item<sub>2</sub> 相似
  * 那么用户很可能喜欢物品 item<sub>2</sub>
* 预估用户对候选物品的兴趣：

  * ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240705151718-2jb375s.png)​
  * 从用户历史行为记录中，我们知道用户对 item<sub>j</sub> 的兴趣，还知道 item<sub>j</sub> 与候选物品的相似度
* 计算两个物品的相似度：

  * 把每个物品表示为一个稀疏向量，向量每个元素对应一个用户
  * 相似度 sim 就是两个向量夹角的余弦

### ItemCF 召回的完整流程

#### 事先做离线计算，建立两个索引

##### 建立“用户 → 物品”的索引

* 记录每个用户最近点击、交互过的物品 ID
* 给定任意用户 ID，可以找到他近期感兴趣的物品列表
* ![1672042313621-2184d04d-8137-4918-84eb-74e4c5fc5ba8](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042313621-2184d04d-8137-4918-84eb-74e4c5fc5ba8-20240705151040-vgs5k6a.png)​

##### 建立“物品 → 物品”的索引

* 计算物品之间两两相似度
* 对于每个物品，索引它最相似的 k 个物品
* 给定任意物品 ID，可以快速找到它最相似的 k 个物品
* ![1672042313734-e3454655-6a02-4b5a-acda-b377d25bb7a7](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042313734-e3454655-6a02-4b5a-acda-b377d25bb7a7-20240705151040-hq8v017.png)​

#### 线上做召回

1. 给定用户 ID，通过“用户 → 物品”索引，找到用户近期感兴趣的物品列表（last-n）
2. 对于 last-n 列表中每个物品，通过“物品 → 物品”的索引，找到 top-k 相似物品
3. 对于取回的相似物品（最多有 𝑛𝑘 个），用公式预估用户对物品的兴趣分数
4. 返回分数最高的 100 个物品，作为推荐结果

* 索引的意义在于避免枚举所有的物品。用索引，离线计算量大，线上计算量小

  1. 记录用户最近感兴趣的 n = 200 个物品
  2. 取回每个物品最相似的 k = 10 个物品
  3. 给取回的 nk = 2000 个物品打分（用户对物品的兴趣）
  4. 返回分数最高的 100 个物品作为 ItemCF 通道的输出

![1672042313852-1dfbf2a7-d136-45da-ae13-711b177a2bf9](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042313852-1dfbf2a7-d136-45da-ae13-711b177a2bf9-20240705151041-5ip3goj.png)

* 如果取回的 item 中有重复的，就去重，并把分数加起来

### 总结

* ItemCF 的原理

  * 用户喜欢物品 i<sub>1</sub> ，那么用户喜欢与物品 i<sub>1</sub> 相似的物品 i<sub>2</sub>
  * 物品相似度：

    * 如果喜欢 i<sub>1</sub>、i<sub>2</sub> 的用户有很大的重叠，那么 i<sub>1</sub> 与 i<sub>2</sub> 相似

      * 不是根据物品内容来判定相似，而使用用户行为来判定
    * 相似公式：![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240705152059-dzn7i6k.png)​

      * W<sub>1</sub> 表示喜欢物品 i<sub>1</sub> 的用户集合
* ItemCF 召回通道

  * 维护两个索引：

    * 用户 → 物品列表：用户最近交互过的 n 个物品
    * 物品 → 物品列表：相似度最高的 k 个物品
* 线上做召回：

  * 利用两个索引，每次取回 nk 个物品
  * 预估用户对每个物品的兴趣分数：![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240705152300-9w0kjx9.png)​
  * 返回分数最高的 100 个物品，作为召回结果

## Swing 召回通道

* Swing 是 ItemCF 的一个变体，在工业界很常用
* Swing 和 ItemCF 很像，唯一区别在于如何定义相似度

### ItemCF

* ItemCF 的原理

  * 物品相似度：如果喜欢 i<sub>1</sub>、i<sub>2</sub>的用户有很大的重叠，那么 i<sub>1</sub> 与 i<sub>2</sub> 相似
  * ![1672042313961-fe69054a-87be-4768-8800-12628b2d93b0](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042313961-fe69054a-87be-4768-8800-12628b2d93b0-20240705151041-5fqy0us.png)​
* ItemCF 的物品相似度

  * 喜欢物品 i<sub>1</sub> 的用户记作集合 W<sub>1</sub>
  * 喜欢物品 i<sub>2</sub> 的用户记作集合 W<sub>2</sub>
  * 定义交集 $V=W  1   ∩W  2  $
  * 两个物品的相似度：  
    ​![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708141038-c2shy9t.png)​

![1672042314110-70b5d35d-1e00-4cb7-8c65-f5da4e19b044](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042314110-70b5d35d-1e00-4cb7-8c65-f5da4e19b044-20240705151042-i1ogms4.png "ItemCF 的物品相似度")​

* ItemCF 的不足之处：

  * 下图中两篇笔记被碰巧分享到了一个微信群里面
  * 造成问题：两篇笔记的受众完全不同，但很多用户同时交互过这两篇笔记，导致系统错误判断两篇笔记相似度很高
  * ![1672042314248-0ccb70c2-f375-43b1-9eab-00dff8aa2ee5](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042314248-0ccb70c2-f375-43b1-9eab-00dff8aa2ee5-20240705151042-p69usy9.png "假如重合的用户是一个小圈子⋯⋯")​
  * 想要解决该问题，就要降低小圈子用户的权重
  * 如果大量不相关的用户同时交互两个物品，则说明两个物品的受众真的相同

### Swing 模型

* 用户 u<sub>1</sub> 喜欢的物品记作集合 J<sub>1</sub>
* 用户 u<sub>2</sub> 喜欢的物品记作集合J<sub>2</sub>
* 定义两个用户的重合度：  
  ​![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708141152-fyszzz5.png)​
* 用户 u<sub>1</sub> 和 u<sub>2</sub> 的重合度高，则他们可能来自一个小圈子，要降低他们的权重
* 喜欢物品 i<sub>1</sub> 的用户记作集合 W<sub>1</sub>
* 喜欢物品 i<sub>2</sub> 的用户记作集合 W<sub>2</sub>
* 定义交集  
  $V=W  1   ∩W  2  $
* 两个物品的相似度：![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708141356-knq955v.png)​

  * α 是超参数
  * $overlap(u1,u2)$表示两个用户的重合度
* 重合度高，说明两人是一个小圈子的，那么他两对物品相似度的贡献就比较小
* 重合度小，两人不是一个小圈子的，他两对物品相似度的贡献就比较大

### 总结

* Swing 与 ItemCF 唯一的区别在于物品相似度
* ItemCF：两个物品重合的用户比例高，则判定两个物品相似
* Swing：额外考虑重合的用户是否来自一个小圈子

  * 同时喜欢两个物品的用户记作集合 V
  * 对于 \mathcal{V} 中的用户 u<sub>1</sub> 和 u<sub>2</sub>，重合度记作 $overlap(u1,u2)$
  * 两个用户重合度大，则可能来自一个小圈子，权重降低

## 基于用户的协同过滤（UserCF）

### UserCF 的原理

![1672042314359-f5bf89fa-b169-46f2-9fbf-6340d7a19628](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042314359-f5bf89fa-b169-46f2-9fbf-6340d7a19628-20240705151042-gl28zje.png)​

* 推荐系统如何找到跟我兴趣非常相似的网友呢？

  * 方法一：点击、点赞、收藏、转发的笔记有很大的重合
  * 方法二：关注的作者有很大的重合

### UserCF 的实现

![1672042314472-e20a2335-b419-46ee-a1cb-7d3ee22eff3d](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042314472-e20a2335-b419-46ee-a1cb-7d3ee22eff3d-20240705151042-rntdf8k.png)​

* 预估用户对候选物品的兴趣：![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708141725-00nq2rc.png)​

  * 例如：0.9×0 + 0.7×1 + 0.7×3 + 0.4×0 = 2.8

### 用户的相似度

![1672042314596-75d3cde7-1e46-45f6-9305-dcd90f827ab5](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042314596-75d3cde7-1e46-45f6-9305-dcd90f827ab5-20240705151043-bzlbnq8.png "两个用户不相似")​![1672042314692-9adddad2-c388-44eb-b1ed-ebf7e468b0c9](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042314692-9adddad2-c388-44eb-b1ed-ebf7e468b0c9-20240705151043-70mlejo.png "两个用户相似")​

* 计算用户相似度

  * 用户 u<sub>1</sub> 喜欢的物品记作集合 J<sub>1</sub>
  * 用户 u<sub>2</sub> 喜欢的物品记作集合 J<sub>2</sub>
  * 定义交集 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708141857-v9bhfc5.png)​
  * 两个用户的相似度：  
    ​![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708141906-k60hzo3.png)​

![1672042315013-ce5b75bd-454a-40c2-9251-63bc9f6279c9](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315013-ce5b75bd-454a-40c2-9251-63bc9f6279c9-20240705151043-sn4d7ol.png)​

* 越热门的物品，越无法反映用户独特的兴趣，对计算用户相似度越没有价值

之前计算相似度时 ![1672042315118-0828a696-f8d5-402a-8913-142c51349cb0](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315118-0828a696-f8d5-402a-8913-142c51349cb0-20240705151044-exc0ony.png)​

* 降低热门物品权重后：![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708141930-e709iwe.png)​

  * n<sub>l</sub>：喜欢物品 l 的用户数量，反映物品的热门程度
  * 物品越热门，$1/log(1+nl   )   $ 越小，对相似度的贡献就越小
* 小结：

  * UserCF 的基本思想：

    * 如果用户 user1 跟用户 user2 相似，而且 user2 喜欢某物品
    * 那么用户 user1 也很可能喜欢该物品
  * 预估用户 user 对候选物品 item 的兴趣：  
    ​![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708142108-fqxgcan.png)​
* 计算两个用户的相似度：
* 把每个用户表示为一个稀疏向量，向量每个元素对应一个物品
* 相似度 sim 就是两个向量夹角的余弦

### UserCF 召回的完整流程

* 事先做离线计算

  * 建立“用户 → 物品”的索引
* 记录每个用户最近点击、交互过的物品 ID
* 给定任意用户 ID，可以找到他近期感兴趣的物品列表
* 建立“用户 → 用户”的索引
* 对于每个用户，索引他最相似的 k 个用户
* 给定任意用户 ID，可以快速找到他最相似的 k 个用户

![1672042315208-7cd72b7f-013a-4175-aeaf-639c69420a95](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315208-7cd72b7f-013a-4175-aeaf-639c69420a95-20240705151044-pd497zt.png "“用户 → 用户”的索引")​![1672042315294-05ee2d9c-ff2f-4823-a139-2e262543263a](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315294-05ee2d9c-ff2f-4823-a139-2e262543263a-20240705151044-x2yvw6j.png "“用户 → 物品”的索引")​

* 线上做召回

  1. 给定用户 ID，通过“用户 → 用户”索引，找到 top-k 相似用户
  2. 对于每个 top-k 相似用户，通过“用户 → 物品”索引，找到用户近期感兴趣的物品列表（last-n）
  3. 对于取回的 nk 个相似物品，用公式预估用户对每个物品的兴趣分数
  4. 返回分数最高的 100 个物品，作为召回结果

![1672042315385-b370a132-0bcd-4714-8149-c508af24d51f](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315385-b370a132-0bcd-4714-8149-c508af24d51f-20240705151044-x23zcpf.png "线上做召回")​

### 总结

* UserCF 的原理：

  * 用户 u1 跟用户 u2 相似，而且 u2 喜欢某物品，那么用户 u1 也很可能喜欢该物品
  * 用户相似度：
* 如果用户 u1 和 u2 喜欢的物品又很大的重叠，那么 u1 和 u2 相似
* 公式：![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708142216-oaemp5p.png)​
* UserCF 召回通道
* 维护两个索引：

  * 用户 → 物品列表：用户近期交互过的 n 个物品
  * 用户 → 用户列表：相似度最高的 k 个用户
* 线上做召回：

  * 利用两个索引，每次取回 𝑛𝑘 个物品
  * 预估用户 user 对每个物品 item 的兴趣分数：  
    ​![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708142231-2nd3z5f.png)​
  * 返回分数最高的 100 个物品，作为召回结果

## 离散特征处理

后面几节讲向量召回，需要用到这部分知识。

### 离散特征

* 举例：

  * 性别：男、女两种类别
  * 国籍：中国、美国、印度等 200 个国家
  * 英文单词：常见的英文单词有几万个
  * 物品 ID：小红书有几亿篇笔记，每篇笔记有一个 ID
  * 用户 ID：小红书有几亿个用户，每个用户有一个 ID
* 离散特征处理的步骤

  1. 建立字典：把类别映射成序号

      * 中国 → 1
      * 美国 → 2
      * 印度 → 3
  2. 向量化：把序号映射成向量

      * One-hot 编码：把序号映射成高维稀疏向量（向量中只有 0，1）
      * Embedding：把序号映射成低维稠密向量（向量中有小数）

### One-Hot 编码

* 例 1：性别特征

  * 性别：男、女两种类别
  * 字典：男 → 1，女 → 2
  * One-hot 编码：用 2 维向量表示性别

    * 未知 → 0 → [0, 0]
    * 男    → 1 → [1, 0]
    * 女    → 2 → [0, 1]
* 例 2：国籍特征

  * 国籍：中国、美国、印度等 200 种类别
  * 字典：中国 → 1，美国 → 2，印度 → 3， ...
  * One-hot 编码：用 200 维稀疏向量表示国籍

    * 未知 → 0 → [0, 0, 0, 0, ⋯ , 0]
    * 中国 → 1 → [1, 0, 0, 0, ⋯ , 0]
    * 美国 → 2 → [0, 1, 0, 0, ⋯ , 0]
    * 印度 → 3 → [0, 0, 1, 0, ⋯ , 0]
* One-Hot 编码的局限

  * 例 1：自然语言处理中，对单词做编码

    * 英文有几万个常见单词
    * 那么 one-hot 向量的维度是几万，实践中一般不会用这么高纬的向量
  * 例 2：推荐系统中，对物品 ID 做编码

    * 小红书有几亿篇笔记
    * 那么 one-hot 向量的维度是几亿
  * 类别数量太大时，通常不用 one-hot 编码

### Embedding（嵌入）

![1672042315474-794bda3a-8cb3-4fbc-b68b-54d9517efbfb](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315474-794bda3a-8cb3-4fbc-b68b-54d9517efbfb-20240705151044-kvo7kdx.png "例1：国籍的 Embedding")

* 参数数量：向量维度 × 类别数量

  * 设 embedding 得到的向量都是 4 维的
  * 一共有 200 个国籍
  * 参数数量 = 4 × 200 = 800
* 编程实现：TensorFlow、PyTorch 提供 embedding 层

  * 参数以矩阵的形式保存，矩阵大小是 向量维度 × 类别数量
  * 输入是序号，比如“美国”的序号是 2
  * 输出是向量，比如“美国”对应参数矩阵的第 2 列
* 例 2：物品 ID 的 Embedding

  * 数据库里一共有 10,000 部电影
  * 任务是给用户推荐电影
  * 设 embedding 向量的维度是 16
  * Embedding 层有多少参数？

    * 参数数量 = 向量维度 × 类别数量 = 160,000
* 一个神经网络绝大多数的参数都在 Embedding 层
* 所以工业界都会对 Embedding 层做很多优化，以提高存储和计算的效率

![1672042315571-3c0c56b8-4ddf-492e-9b67-e709beaf7df3](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315571-3c0c56b8-4ddf-492e-9b67-e709beaf7df3-20240705151045-jqabqfx.png "相似物品的 Embedding 距离更近")

![1672042315677-b4f95767-1df5-4f3e-9827-eaef2abb582c](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315677-b4f95767-1df5-4f3e-9827-eaef2abb582c-20240705151045-zqku7sx.png "Embedding = 参数矩阵 × One-Hot 向量")

### 总结

* 离散特征处理：one-hot 编码、embedding
* 类别数量很大时，用 embedding

  * Word embedding
  * 用户 ID embedding
  * 物品 ID embedding

## 矩阵补充

* 矩阵补充是向量召回最常见的方法，只是现在不常用了
* 此处讲解矩阵补充，是为了帮助理解下节课的双塔模型

![1672042315791-5bfc7658-6112-4537-8a28-4d1085472232](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315791-5bfc7658-6112-4537-8a28-4d1085472232-20240705151045-o8f7k4l.png "矩阵补充模型")

矩阵补充模型：基于 Embedding 来做推荐。

### 训练

* 基本想法

  * 用户 embedding 参数矩阵记作 A。

    * 第 u 号用户对应矩阵第 u 列，记作向量 a<sub>u</sub>
  * 物品 embedding 参数矩阵记作 B。

    * 第 i 号物品对应矩阵第 i 列，记作向量 b<sub>i</sub>
  * 内积  ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708144426-8c1dui6.png)是第 u 号用户对第 i 号物品兴趣的预估值
  * 训练模型的目的是学习矩阵 A 和 B，使得预估值拟合真实观测的兴趣分数

![1672042315908-e52971dc-adb4-4fe8-a7cd-95e25d7703fc](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042315908-e52971dc-adb4-4fe8-a7cd-95e25d7703fc-20240705151045-vqrc1le.png)​

* 数据集

  * 数据集：（用户 ID，物品 ID，真实兴趣分数）的集合，记作 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708144456-8p8yvsv.png)​
  * 数据集中的兴趣分数是系统记录的，比如：

    * 曝光但是没有点击 → 0 分
    * 点击、点赞、收藏、转发 → 各算 1 分
    * 分数最低是 0，最高是 4
  * 训练的目的就是让模型的输出拟合真实兴趣分数
* 训练

  * 把用户 ID、物品 ID 映射成向量。

    * 第 u 号用户 → 向量 a<sub>u</sub>
    * 第 i 号物品 → 向量 b<sub>i</sub>
  * 求解优化问题，得到参数 A 和 B ![image](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20240708144613-uktcil8.png)​

    * 找到使得 真实兴趣分数 y 与 模型输出 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708144632-6o0eox7.png) 间 差别 最小的 A 和 B
    * 求最小化常用的方法就是随机梯度下降，每次更新矩阵 A 和 B 的一列
* 解释下为什么模型叫做矩阵补充

  * ![1672042316018-1bf99117-9105-4290-9997-5fd589d5ba94](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042316018-1bf99117-9105-4290-9997-5fd589d5ba94-20240705151045-zny4kp8.png)​

    * 绿色位置表示曝光给用户的物品；灰色位置表示没有曝光
  * 矩阵中只有少数位置是绿色，大多数位置是灰色（即大部分物品没有曝光给用户）
  * 而我们用绿色位置训练出的模型，可以预估所有灰色位置的输出，即把矩阵的元素补全
  * 把矩阵元素补全后，我们只需选出对应用户一行中分数较高的 物品 推荐给 用户 即可
* 矩阵补充在实践中效果不好 ……

  * 缺点 1：仅用 ID embedding，没利用物品、用户属性

    * 物品属性：类目、关键词、地理位置、作者信息
    * 用户属性：性别、年龄、地理定位、感兴趣的类目
    * 双塔模型可以看做矩阵补充的升级版

      * 双塔模型不仅使用 ID，还结合各种属性
  * 缺点 2：负样本的选取方式不对

    * 样本：用户—物品的二元组，记作 (u,i)
    * 正样本：曝光之后，有点击、交互。（正确的做法）
    * 负样本：曝光之后，没有点击、交互。（错误的做法）

      * 后面会专门用一节课时间讲正负样本如何选择
  * 缺点 3：做训练的方法不好

    * 内积 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708144826-rjeb9wg.png) 不如余弦相似度

      * 工业界普遍使用 余弦相似度 而不是 内积
    * 用平方损失（回归），不如用交叉熵损失（分类）

### 线上服务

* 模型存储

  1. 训练得到矩阵 A 和 B

      * A 的每一列对应一个用户
      * B 的每一列对应一个物品
  2. 把矩阵 A 的列存储到 key-value 表

      * key 是用户 ID，value 是 A 的一列
      * 给定用户 ID，返回一个向量（用户的 embedding）
  3. 矩阵 B 的存储和索引比较复杂

* 线上服务

  1. 把用户 ID 作为 key，查询 key-value 表，得到该用户的 embedding 向量，记作 a
  2. 最近邻查找：查找用户最有可能感兴趣的 k 个物品，作为召回结果

      1. 第 𝑖 号物品的 embedding 向量记作 b<sub>i</sub>
      2. 内积 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708145128-rmj2xap.png) 是用户对第 𝑖 号物品兴趣的预估
      3. 返回内积最大的 k 个物品
* 如果枚举所有物品，时间复杂度正比于物品数量
* 最近邻查找的计算量太大，不现实，下面讲解如何加速最近邻查找

### 近似最近邻查找

* 支持最近邻查找的系统

  * 系统：Milvus、Faiss、HnswLib、等等

    * 快速最近邻查找的算法已经被集成到这些系统中
  * 衡量最近邻的标准：

    * 欧式距离最小（L2 距离）
    * 向量内积最大（内积相似度）

      * 矩阵补充用的就是内积相似度
    * 向量夹角余弦最大（cosine 相似度）

      * 最常用
      * 对于不支持的系统：把所有向量作归一化（让它们的二范数等于 1），此时内积就等于余弦相似度

![1672042316143-7b94cbd6-db64-4ce6-8739-b7070b471431](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042316143-7b94cbd6-db64-4ce6-8739-b7070b471431-20240705151045-wbhztev.png)

* 图中每个点表示一个物品的 Embedding
* 右边的 ★ 表示用户 a
* 数据预处理：把数据据划分为多个区域

  * 划分后，每个区域用一个向量表示，这些向量的长度都是 1
  * 例如图中蓝色区域用蓝色箭头向量表示
  * ![1672042316265-f9bdddd9-6a26-4c0c-b9cb-033cf5e8b18d](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042316265-f9bdddd9-6a26-4c0c-b9cb-033cf5e8b18d-20240705151045-6vtkcqt.png)​
* 直至每个区域都用一个单位向量来表示

  * 这些单位向量又称为 索引向量
  * ![1672042316397-ffc052b7-a1d1-49fa-bbae-d766582e91e2](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042316397-ffc052b7-a1d1-49fa-bbae-d766582e91e2-20240705151046-wcf8kkw.png)​
* 实际推荐时，先把用户向量与所有的索引向量做对比

  * 计算相似度，找到最相似的索引向量
  * ![1672042316503-39e31422-ab4e-4d1f-9c9c-95c492ed675d](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042316503-39e31422-ab4e-4d1f-9c9c-95c492ed675d-20240705151046-m86advq.png)​
* 通过索引向量，我们找到索引对应区域中的所有物品，然后再计算该区域中所有物品与 a 的相似度

  * 一般情况是几亿个物品被几万个索引向量划分，于是一个区域中就只有几万个物品
  * ![1672042316599-f6f2d111-6db1-40cf-adee-f7183ad8ef92](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042316599-f6f2d111-6db1-40cf-adee-f7183ad8ef92-20240705151046-3qj5he8.png)​

### 总结

* 矩阵补充

  * 把物品 ID、用户 ID 做 embedding，映射成向量
  * 两个向量的内积![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708145431-fn4lw3i.png) 作为用户 u 对物品 i 兴趣的预估值
  * 训练时让 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708145431-fn4lw3i.png) 拟合真实观测的兴趣分数，学习模型的 embedding 层参数
  * 矩阵补充模型有很多缺点，效果不好
* 线上召回

  * 把用户向量 a 作为 query，查找使得 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708145431-fn4lw3i.png)  最大化的物品 i
  * 暴力枚举速度太慢。实践中用近似最近邻查找
  * Milvus、Faiss、HnswLib 等向量数据库支持近似最近邻查找

## 双塔模型：模型和训练

![1672042316690-403d8462-f026-4de3-a43e-d99119362d01](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042316690-403d8462-f026-4de3-a43e-d99119362d01-20240705151046-swdfyzy.png)

### 双塔模型

![1672042316812-3bce2474-8eda-48cb-aa80-6f58bb78b238](https://gitee.com/JBL_lun/tuchuang/raw/master/assets/net-img-1672042316812-3bce2474-8eda-48cb-aa80-6f58bb78b238-20240705151046-zxmajg9.png)​![1672042316930-512c4448-5964-4bf2-9309-419cd3a16db5](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042316930-512c4448-5964-4bf2-9309-419cd3a16db5-20240705151046-7e62e2q.png)

* 用户离散特征：例如所在城市、感兴趣的话题等

  * 对每个离散特征，单独使用一个 Embedding 层得到一个向量
  * 对于性别这种类别很少的离散特征，直接用 one-hot 编码
* 用户连续特征：年龄、活跃程度、消费金额等

![1672042317078-7e47cf8c-e659-4582-8ea5-4b131b39913f](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042317078-7e47cf8c-e659-4582-8ea5-4b131b39913f-20240705151047-ckzime3.png "双塔模型")

* 双塔模型：左塔提取用户特征，右塔提取物品特征

  * 与矩阵补充的区别在于，使用了除 ID 外的多种特征作为双塔的输入
* 双塔模型的训练

  * Pointwise：独立看待每个正样本、负样本，做简单的二元分类
  * Pairwise：每次取一个正样本、一个负样本<sub>[1]</sub>
  * Listwise：每次取一个正样本、多个负样本<sub>[2]</sub>

* 正负样本的选择

  * 正样本：用户点击的物品
  * 负样本<sub>[1,2]</sub>：

    * 没有被召回的？
    * 召回但是被粗排、精排淘汰的？
    * 曝光但是未点击的？

### Pointwise 训练

* 把召回看做二元分类任务
* 对于正样本，鼓励 $cos(a,b)$ 接近 +1
* 对于负样本，鼓励 $cos(a,b)$ 接近 −1
* 控制正负样本数量为 1: 2 或者 1: 3

### Pairwise 训练

![1672042317230-6a26dfa7-08dd-4a8b-a7c0-532855e6168f](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042317230-6a26dfa7-08dd-4a8b-a7c0-532855e6168f-20240705151047-7ru833g.png)

* 两个物品塔是相同的，它们共享参数

![1672042317349-25d382e4-8c3f-4881-984e-7348d3173963](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042317349-25d382e4-8c3f-4881-984e-7348d3173963-20240705151047-mo0luci.png)

* 基本想法：鼓励  ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708151946-7vjhx3f.png)大于 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152001-fq2hcus.png)​

  * 如果  ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152023-nmvgarp.png)大于  ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152001-fq2hcus.png)+m，则没有损失

    * m 是超参数，需要调
  * 否则，损失等于  ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152050-v8v4mnh.png)​
* **Triplet hinge loss**:

  * ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152101-ykih6n7.png)​
* **Triplet logistic loss:**

  * ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152112-88nmdja.png)​
* σ 是大于 0 的超参数，控制损失函数的形状，需手动设置

### Listwise 训练

* 一条数据包含：

  * 一个用户，特征向量记作 a
  * 一个正样本，特征向量记作 b<sup>+</sup>
  * 多个负样本，特征向量记作 b<sub>1</sub>​<sub><sup>+</sup></sub>    ，...，b<sub>n</sub>​<sub><sup>+</sup></sub>
* 鼓励 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152023-nmvgarp.png) 尽量大
* 鼓励 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152256-9wttl6w.png)尽量小

![1672042317453-2e6860cf-79ad-492a-b350-5f8115174edf](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042317453-2e6860cf-79ad-492a-b350-5f8115174edf-20240705151047-b4o62va.png "Listwise 训练")

* 正样本 y<sup>+</sup> = 1，即鼓励 s<sup>+</sup> 趋于 1
* 负样本 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152356-fa7y5il.png)，即鼓励 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152404-shnfdil.png) 趋于 0
* 用 y 和 s 的交叉熵作为损失函数，意思是鼓励 $Softmax$ 的输出 s 接近标签 y

### 总结

* 双塔模型

  * 用户塔、物品塔各输出一个向量
  * 两个向量的余弦相似度作为兴趣的预估值
  * 三种训练方式：

    * Pointwise：每次用一个用户、一个物品（可正可负）
    * Pairwise：每次用一个用户、一个正样本、一个负样本
    * Listwise：每次用一个用户、一个正样本、多个负样本
* 不适用于召回的模型

  * ![1672042317590-889ebdfc-4e88-4b03-bfd7-57e946cbdc3f](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042317590-889ebdfc-4e88-4b03-bfd7-57e946cbdc3f-20240705151047-ztc3taw.png)​
  * 用户和物品的向量在进入神经网络前就拼接起来了，和双塔模型有很大区别

    * 双塔模型是在后期输出相似度时才进行融合
    * 用户（或物品）自身特征的拼接没有影响，依然保持了用户（或物品）的独立性

      * 而一旦用户和物品进行拼接，此时的输出就特定于该 用户（或物品）了
  * 这种前期融合的模型，不适用于召回

    * 因为得在召回前，把每个用户向量对应的所有物品向量挨个拼接了送入神经网络
    * 假设有一亿个物品，每给用户做一次召回，就得跑一亿遍
  * 这种模型通常用于排序，在几千个候选物品中选出几百个
  * 以后看到这种模型就要意识到 —— 这是排序模型，不是召回模型

> 采用双塔模型这种后期融合的召回方式，其优点在于可以借助物品塔计算好所有物品的表示。之后每次来一个用户，将其通过用户塔可以得到他的表示，借助上一节课讲到的快速最近邻的方法，可以快速召回k个和用户表示相近的物品。
>
> 而如果采用前期融合的方式，就无法预先计算好所有物品的表示，要召回k个物品，必须让用户的特征和每个物品的特征融合后输入神经网络得到一个感兴趣分数，这样计算量会非常大，发挥不了快速最近邻的方法的优势。

## 双塔模型：正负样本

### 正样本

* 正样本：曝光而且有点击的 用户—物品 二元组（用户对物品感兴趣）
* 问题：少部分物品占据大部分点击，导致正样本大多是热门物品
* 解决方案：过采样冷门物品，或降采样热门物品

  * 过采样（up-sampling）：一个样本出现多次
  * 降采样（down-sampling）：一些样本被抛弃
* 以一定概率抛弃热门物品，抛弃的概率与样本的点击次数正相关

![1672042317744-98f7f9f0-abad-4634-9feb-2f567ab0f9fd](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042317744-98f7f9f0-abad-4634-9feb-2f567ab0f9fd-20240705151047-0ut2jw0.png)

![1672042317852-e1721f7f-f1cb-4910-af2f-0847eaf52b1b](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042317852-e1721f7f-f1cb-4910-af2f-0847eaf52b1b-20240705151048-emb35jy.png "如何选择负样本？")

### 简单负样本

* 简单负样本：全体物品

  * 未被召回的物品，大概率是用户不感兴趣的
  * 未被召回的物品 ≈ 全体物品
  * 从全体物品中做抽样，作为负样本
  * 均匀抽样 or 非均匀抽样？
* 均匀抽样：对冷门物品不公平

  * 正样本大多是热门物品
  * 如果均匀抽样产生负样本，负样本大多是冷门物品
* 非均抽采样：目的是打压热门物品

  * 负样本抽样概率与热门程度（点击次数）正相关
  * 抽样概率 ∝ (点击次数)<sup>0.75</sup>

    * ∝：正比于
* 简单负样本：Batch 内负样本

  * ![1672042318032-e4d044d2-fbe2-48c6-9187-9e896e7e7016](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042318032-e4d044d2-fbe2-48c6-9187-9e896e7e7016-20240705151048-d92et9q.png)​![1672042318149-0b62c2f5-9601-4cf0-a8ba-b1df4875ddd0](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042318149-0b62c2f5-9601-4cf0-a8ba-b1df4875ddd0-20240705151048-mj2090i.png)​
  * 一个 batch 内有 n 个正样本
  * 一个用户和 n-1 个物品组成负样本
  * 这个 batch 内一共有 n(n-1) 个负样本
  * 都是简单负样本。（因为第一个用户不喜欢第二个物品）
  * Batch 内负样本存在的问题

    * 一个物品出现在 batch 内的概率 ∝ 点击次数
    * 物品成为负样本的概率本该是 ∝  (点击次数)<sup>0.75</sup>，但这里实际是 ∝ 点击次数的一次方
    * 热门物品成为负样本的概率过大

      * 即对热门物品打压太狠了，容易造成偏差

* 修正偏差：

  * 物品 i 被抽样到的概率：p<sub>i</sub> ∝ 点击次数
  * 预估用户对物品 i 的兴趣：![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152823-0ybgju5.png)​
  * 做训练的时候，将兴趣调整为：![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152830-f65e6a3.png)​

    * 这样纠偏，避免过度打压热门物品
    * 训练结束后，在线上做召回时，还是用 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240708152844-mw74h31.png) 作为兴趣

### 困难负样本

* 困难负样本：

  * 被粗排淘汰的物品（比较困难）

    * 这些物品被召回，说明和用户兴趣有关；又被粗排淘汰，说明用户对物品兴趣不大
    * 而在对正负样本做二元分类时，这些困难样本容易被分错（被错误判定为正样本）
  * 精排分数靠后的物品（非常困难）

    * 能够进入精排，说明物品比较符合用户兴趣，但不是用户最感兴趣的
* 对正负样本做二元分类：

  * 全体物品（简单）分类准确率高
  * 被粗排淘汰的物品（比较困难）容易分错
  * 精排分数靠后的物品（非常困难）更容易分错
* 训练数据

  * 混合几种负样本
  * 50% 的负样本是全体物品（简单负样本）
  * 50% 的负样本是没通过排序的物品（困难负样本）

    * 即在粗排、精排淘汰的物品

### 常见的错误

![1672042318236-d34cc538-6373-4de9-a53c-21c2eb6173a6](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042318236-d34cc538-6373-4de9-a53c-21c2eb6173a6-20240705151048-ujk6gn4.png "曝光但是没有点击")

> 曝光但是没有点击的样本，不能作为召回的负样本，但是可以作为排序的负样本

* 选择负样本的原理

  * 召回的目标：快速找到用户可能感兴趣的物品

    * 即区分用户 **不感兴趣** 和 **可能感兴趣** 的物品，而不是区分 **比较感兴趣** 和 **非常感兴趣** 的物品
  * 全体物品（easy ）：绝大多数是用户根本不感兴趣的
  * 被排序淘汰（hard ）：用户可能感兴趣，但是不够感兴趣
  * 有曝光没点击（没用）：用户感兴趣，可能碰巧没有点击

    * 曝光没点击的物品已经非常符合用户兴趣了，甚至可以拿来做召回的正样本
    * 可以作为排序的负样本，不能作为召回的负样本

### 总结

* 正样本： 曝光而且有点击
* 简单负样本：

  * 全体物品
  * Batch 内负样本
* 困难负样本：被召回，但是被排序淘汰
* 错误：曝光、但是未点击的物品做召回的负样本

## 双塔模型：线上召回和更新

### 线上召回

![1672042318339-77721c1a-e3b9-4240-a57b-479dc9417287](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042318339-77721c1a-e3b9-4240-a57b-479dc9417287-20240705151048-ej93wz7.png "离线存储")​![1672042318449-365243b3-52b6-4ad2-ac6f-7fe09a5854a9](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042318449-365243b3-52b6-4ad2-ac6f-7fe09a5854a9-20240705151048-25ve006.png "线上召回")

* 双塔模型的召回

  * 离线存储：把物品向量 b 存入向量数据库

    * 完成训练之后，用物品塔计算每个物品的特征向量 b
    * 把几亿个物品向量 b 存入向量数据库（比如 Milvus、Faiss、HnswLib ）
    * 向量数据库建索引，以便加速最近邻查找
  * 线上召回：查找用户最感兴趣的 k 个物品

    * 给定用户 ID 和画像，线上用神经网络现算（**实时计算**）用户向量 a
    * 最近邻查找：

      * 把向量 a 作为 query，调用向量数据库做最近邻查找
      * 返回余弦相似度最大的 k 个物品，作为召回结果

* 为什么事先存储物品向量 b，线上现算用户向量 a？

  * 每做一次召回，用到一个用户向量 a，几亿物品向量 b（线上算物品向量的代价过大）
  * 用户兴趣动态变化，而物品特征相对稳定（可以离线存储用户向量，但不利于推荐效果）

### 模型更新

* 全量更新 vs 增量更新

  * 全量更新：今天凌晨，用昨天全天的数据训练模型

    * 在昨天模型参数的基础上做训练（不是重新随机初始化）
    * 用昨天的数据，训练 1 epoch，即每天数据只用一遍
    * 发布新的 用户塔神经网络 和 物品向量，供线上召回使用
    * 全量更新对数据流、系统的要求比较低
  * 增量更新：做 online learning 更新模型参数

    * 用户兴趣会随时发生变化
    * 实时收集线上数据，做流式处理，生成 TFRecord 文件
    * 对模型做 online learning，增量更新 ID Embedding 参数（不更新神经网络其他部分的参数）

      * 即锁住全连接层的参数，只更新 Embedding 层的参数，这是出于工程实现的考量
    * 发布用户 ID Embedding，供用户塔在线上计算用户向量

![1672042318550-93ac8230-4a4e-44b0-824a-852475c1925d](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042318550-93ac8230-4a4e-44b0-824a-852475c1925d-20240705151049-3nt4ocm.png)

![1672042318647-8b5ba9e7-e9a1-4ce8-bb89-5d9a14fb364e](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042318647-8b5ba9e7-e9a1-4ce8-bb89-5d9a14fb364e-20240705151049-oljdzcv.png)

* 问题：能否只做增量更新，不做全量更新？

  * 小时级数据有偏；分钟级数据偏差更大

    * 在不同时间段，用户行为不一样，这和全体数据的统计值差别很大
  * 全量更新：random shuffle 一天的数据，做 1 epoch 训练

    * random shuffle 消除了不同时间段的差别
  * 增量更新：按照数据从早到晚的顺序，做 1 epoch 训练
  * **随机打乱**优于**按顺序排列数据**，全量训练优于增量训练

### 总结

* 双塔模型

  * 用户塔、物品塔各输出一个向量，两个向量的余弦相似度作为兴趣的预估值
  * 三种训练的方式：pointwise、pairwise、listwise
  * 正样本：用户点击过的物品
  * 负样本：全体物品（简单）、被排序淘汰的物品（困难）
* 召回

  * 做完训练，把物品向量存储到向量数据库，供线上最近邻查找
  * 线上召回时，给定用户 ID、用户画像，调用用户塔现算用户向量 a
  * 把 a 作为 query，查询向量数据库，找到余弦相似度最高的 k 个物品向量，返回 k 个物品 ID
* 更新模型

  * 全量更新：今天凌晨，用昨天的数据训练整个神经网络，做 1 epoch 的随机梯度下降
  * 增量更新：用实时数据训练神经网络，只更新 ID Embedding，锁住全连接层
  * 实际的系统：

    * 全量更新 & 增量更新 相结合
    * 每隔几十分钟，发布最新的用户 ID Embedding，供用户塔在线上计算用户向量

## 自监督学习

### `双塔模型的问题`​

* 推荐系统的头部效应严重:

  * 少部分物品占据大部分点击。 • 大部分物品的点击次数不高。
  * 高点击物品的表征学得好，长尾物品的表征学得不好。 • 自监督学习:做 data augmentation，更好地学习长尾  
    物品的向量表征。

### `自监督学习`​

* 双塔模型学不好低曝光物品的向量表征。
* 自监督学习：

  * 对物品做随机特征变换。
  * 物品 𝑖 的两个向量表征 b<sub>i</sub>​<sub><sup>'</sup></sub> 和 b<sub>i</sub>​<sub><sup>''</sup></sub>: 有较高的相似度（相同物品）。
  * 物品 𝑖 和 𝑗 的向量表征 b<sub>i</sub>​<sub><sup>'</sup></sub> 和 b<sub>j</sub>​<sub><sup>''</sup></sub> 有较低的相似度（不同物品）。
  * 鼓励 cos(b<sub>i</sub>​<sub><sup>'</sup></sub>  , b<sub>i</sub>​<sub><sup>''</sup></sub>) 尽量大， cos(b<sub>i</sub>​<sub><sup>'</sup></sub>  , b<sub>j</sub>​<sub><sup>''</sup></sub>) 尽量小。
* 实验效果:低曝光物品、新物品的推荐变得更准。

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709103846-pvl9pth.png)​

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709103858-yj9nvz8.png)​

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709103918-faw86w2.png)​

### `特征变换`​

特征变换:互补特征(complementary)

* 假设物品一共有 4 种特征:

  * ID，类目，关键词，城市

* 随机分成两组:

  * `{ID，关键词}` 和 `{类目，城市}`

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709104516-3xg5uw8.png)​

## 其他召回通道

### 地理位置召回

* GeoHash 召回

  * 用户可能对附近发生的事感兴趣
  * GeoHash：对经纬度的编码，大致表示地图上一个长方形区域
  * 索引：GeoHash → 优质笔记列表（按时间倒排）
  * 这条召回通道没有个性化

![1672042318740-fbad5f64-88ed-4abf-b630-999a4bd566bb](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1672042318740-fbad5f64-88ed-4abf-b630-999a4bd566bb-20240705151049-3omzytu.png)​

* 根据用户定位的 GeoHash，取回该地点最新的 k 篇优质笔记
* 同城召回

  * 用户可能对同城发生的事感兴趣
  * 索引： 城市 → 优质笔记列表（按时间倒排）
  * 这条召回通道没有个性化

### 作者召回

* 关注作者召回

  * 用户对关注的作者发布的笔记感兴趣
  * 索引：

    * 用户 → 关注的作者
    * 作者 → 发布的笔记
  * 召回：

    * 用户id → 关注的作者 → 最新的笔记
* 有交互的作者召回

  * 如果用户对某笔记感兴趣（点赞、收藏、转发），那么用户可能对该作者的其他笔记感兴趣
  * 索引： 用户 → 有交互的作者

    * 作者列表需要定期更新，加入最新交互的作者，删除长期未交互的作者
  * 召回： 用户id → 有交互的作者 → 最新的笔记
* 相似作者召回

  * 如果用户喜欢某作者，那么用户喜欢相似的作者
  * 索引：作者 → 相似作者（k 个作者）

    * 作者相似度的计算类似于 ItemCF 中判断两个物品的相似度
    * 例如两个作者的粉丝有很大重合，则认定两个作者相似
  * 召回：用户 → 感兴趣的作者 → 相似作者 → 最新的笔记  
    （n 个作者）  （nk 个作者）（nk 篇笔记）

### 缓存召回

* 缓存召回

  * 想法：复用前 n 次推荐精排的结果
  * 背景：

    * 精排输出几百篇笔记，送入重排
    * 重排做多样性抽样，选出几十篇
    * 精排结果一大半没有曝光，被浪费
  * 精排前 50，但是没有曝光的，缓存起来，作为一条召回通道
* 缓存大小固定，需要退场机制

  * 一旦笔记成功曝光，就从缓存退场
  * 如果超出缓存大小，就移除最先进入缓存的笔记
  * 笔记最多被召回 10 次，达到 10 次就退场
  * 每篇笔记最多保存 3 天，达到 3 天就退场

### 总结

* 地理位置召回：

  * GeoHash 召回、同城召回
* 作者召回：

  * 关注的作者、有交互的作者、相似的作者
* 缓存召回

这 6 条召回通道都是工业界在用的，只是它们的重要性比不上 ItemCF、Swing、双塔那些通道。

## 曝光过滤 & Bloom Filter

### 曝光过滤问题

* 如果用户看过某个物品，则不再把该物品曝光给该用户。
* 对于每个用户，记录已经曝光给他的物品。(小红书只召回 1 个月以内的笔记，因此只需要记录每个用户最近 1 个 月的曝光历史。)
* 对于每个召回的物品，判断它是否已经给该用户曝光过， 排除掉曾经曝光过的物品。
* 一位用户看过 𝑛 个物品，本次召回 𝑟 个物品，如果暴力对 比，需要$𝑂(𝑛𝑟)$ 的时间。

### Bloom Filter

* Bloom filter 判断一个物品 ID 是否在已曝光的物品集合中。

  * 如果判断为 no，那么该物品一定不在集合中。
  * 如果判断为 yes，那么该物品很可能在集合中。(可能误伤， 错误判断未曝光物品为已曝光，将其过滤掉。)

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709133634-13hqgio.png)

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709133652-cq4kagi.png)

* 曝光物品集合大小为 𝑛，二进制向量维度为 𝑚，使用 𝑘 个哈 希函数。
* Bloomfilter 误伤的概率为 ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709134230-lr5cym8.png)​

  * 𝑛 越大，向量中的 1 越多，误伤概率越大。(未曝光物品的 𝑘 个位置恰好都是 1 的概率大。)
  * 𝑚 越大，向量越长，越不容易发生哈希碰撞。
  * 𝑘 太大、太小都不好， 𝑘 有最优取值。
  * 设定可容忍的误伤概率为 𝛿，那么最优参数为:

    * ![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709134358-vmy6if2.png)​

### 曝光过滤的链路

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709134444-0objry0.png)

### Bloom Filter的缺点

* bloom filter 把物品的集合表示成一个二进制向量。
* 每往集合中添加一个物品，只需要把向量 𝑘 个位置的元素置为 1。(如果原本就是 1，则不变。)
* Bloom filter 只支持添加物品，不支持删除物品。从集合中移除物品，无法消除它对向量的影响。
* 每天都需要从物品集合中移除年龄大于 1 个月的物品。 (超龄物品不可能被召回，没必要把它们记录在  
  Bloom filter，降低 𝑛 可以降低误伤率。)

# 排序

> * [📎多目标排序模型.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1676619021423-075c82b6-d732-468c-9d4a-f09aa75acbcd.pdf)
> * [📎Multi-gate Mixture-of-Experts MMoE.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1676619021381-5a7ffcf8-bc7a-4d4e-ae5a-d526be46f7aa.pdf)
> * [📎预估分数融合.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1676619019658-0aac3bde-6f79-4827-a7de-2d7f19ca47ff.pdf)
> * [📎播放时长建模.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1676619020125-5800d032-1547-4090-9db8-6803c77418b4.pdf)
> * [📎推荐系统的特征.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1676619019750-f3568649-f28b-42f0-94e8-861d0b9b1e49.pdf)
> * [📎粗排三塔模型.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1676619020734-069bd880-e217-46fc-88ba-93ced388841a.pdf)

## 多目标排序模型

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709140855-m8xz82g.png "推荐系统的链路")

### 用户—笔记的交互

排序的主要依据是用户对笔记的兴趣，而兴趣都反映在 用户—笔记 的交互中。

* 对于每篇笔记，系统记录：

  * 曝光次数（number of impressions）
  * 点击次数（number of clicks）
  * 点赞次数（number of likes）
  * 收藏次数（number of collects）
  * 转发次数（number of shares）
* 点击率 = 点击次数 / 曝光次数
* 点赞率 = 点赞次数 / 点击次数
* 收藏率 = 收藏次数 / 点击次数
* 转发率 = 转发次数 / 点击次数
* 排序的依据

  * 排序模型预估点击率、点赞率、收藏率、转发率等多种分数
  * 融合这些预估分数（比如加权和）
  * 根据融合的分数做排序、截断

### 多目标模型

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709140951-czkoyrg.png)

* 统计特征包括"用户统计特征"和"物品统计特征"
* "场景特征" 是随着用户请求传过来的

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709141015-dfj92q3.png)

* 训练：让预估值接近真实目标值

  * 总的损失函数：![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709141054-nwmd5kh.png)​
  * 对损失函数求梯度，做梯度下降更新参数

    困难：类别不平衡，即正样本数量显著少于负样本

    * 每 100 次曝光，约有 10 次点击、90 次无点击
    * 每 100 次点击，约有 10 次收藏、90 次无收藏
    * 注：不是小红书的真实数据
  * 解决方案：负样本降采样（down-sampling）

    * 保留一小部分负样本
    * 让正负样本数量平衡，节约计算量

## 预估分数的融合

* 简单的加权和

  * ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141329-pbwbo3o.svg)​

* 点击率乘以其他项的加权和

  * ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141344-jo1iwbf.svg)​

    * ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141344-5agog1x.svg)，![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141344-jqm0khb.svg)​
    * 所以 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141344-r81illf.svg)​

* 海外某短视频 APP 的融分公式

  * ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141440-pd1fl9j.svg)​

* 国内某短视频 APP （老铁）的融分公式

  * 根据预估时长 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141440-i6j51mh.svg)，对 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141441-mg3giwb.svg) 篇候选视频做排序
  * 如果某视频排名第 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141441-fayil3v.svg)，则它得分 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141441-3lyp66f.svg)​
  * 对点击、点赞、转发、评论等预估分数做类似处理
  * 最终融合分数： ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141441-8d8vhws.svg)​

    * 公式特点在于 —— 使用预估的排名

* 某电商的融分公式

  * 电商的转化流程：曝光 → 点击 → 加购物车 → 付款
  * 模型预估：![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141441-5cs0ysr.svg)、![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141442-dgyrz04.svg)、![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141442-s182qqd.svg)​
  * 最终融合分数： ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141442-mfkcsgc.svg)​

    * 假如 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709141442-ouwi315.svg) 那该公式就是电商的营收，有明确的物理意义

## 排序模型的特征

### 特征

#### 用户画像（User Profile）

* 用户 ID（在召回、排序中做 embedding）

  * 用户 ID 本身不携带任何信息，但模型学到的 ID embedding 对召回和排序有很重要的影响

* 人口统计学属性：性别、年龄
* 账号信息：新老、活跃度......

  * 模型需要专门针对 新用户 和 低活跃 用户做优化
* 感兴趣的类目、关键词、品牌

#### 物品画像（Item Profile）

* 物品 ID（在召回、排序中做 embedding）

* 发布时间（或者年龄）
* GeoHash（经纬度编码）、所在城市
* 标题、类目、关键词、品牌......
* 字数、图片数、视频清晰度、标签数......

  * 反映笔记的质量
* 内容信息量、图片美学......

  * 事先用人工标注的数据训练 NLP 和 CV 模型，然后用模型打分

#### 用户统计特征

* 用户最近 30 天（7 天、1 天、1 小时）的曝光数、点击数、点赞数、收藏数......

  * 划分各种时间粒度，可以反映用户的 实时、短期、中长期 兴趣
* 按照笔记图文/视频分桶。（比如最近 7 天，该用户对图文笔记的点击率、对视频笔记的点击率）

  * 反映用户对两类笔记的偏好
* 按照笔记类目分桶。（比如最近 30 天，用户对美妆笔记的点击率、对美食笔记的点击率、对科技数码笔记的点击率）

  * 反映用户对哪个类目更感兴趣

#### 笔记统计特征

* 笔记最近 30 天（7 天、1 天、1 小时）的曝光数、点击数、点赞数、收藏数......

  * 划分时间粒度，可以提前发现哪些笔记过时了
* 按照用户性别分桶、按照用户年龄分桶......
* 作者特征：

  * 发布笔记数
  * 粉丝数
  * 消费指标（曝光数、点击数、点赞数、收藏数）

#### 场景特征（Context）

* 用户定位 GeoHash（经纬度编码）、城市
* 当前时刻（分段，做 embedding）

  * 一个人在同一天不同时刻的兴趣是变化的
  * 而且可以反推用户是在上班路上、公司、家里
* 是否是周末、是否是节假日
* 手机品牌、手机型号、操作系统

  * 安卓用户和苹果用户的 点击率、点赞率 等数据差异很大

#### 特征处理

* 离散特征：做 embedding

  * 用户 ID、笔记 ID、作者 ID
  * 类目、关键词、城市、手机品牌
* 连续特征：

  * 做分桶，变成离散特征

    * 年龄、笔记字数、视频长度
    * 比如把连续的年龄分为10个年龄段，做 onehot编码 或 embedding
  * 其他变换

    * 曝光数、点击数、点赞数等数值做 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709142107-ljppi9u.svg)​
    * 转化为点击率、点赞率等值，并做平滑

#### 特征覆盖率

* 很多特征无法覆盖 100% 样本

  * 例：很多用户不填年龄，因此用户年龄特征的覆盖率远小于 100%
  * 例：很多用户设置隐私权限，APP 不能获得用户地理定位，因此场景特征有缺失

* 提高特征覆盖率，可以让精排模型更准

  * 想各种办法提高特征覆盖率，并考虑特征缺失时默认值如何设置

### 数据服务

1. 用户画像（User Profile）
2. 物品画像（Item Profile）
3. 统计数据

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709142227-lek0qwd.png)

* 用户画像数据库压力小（每次只读 1 个用户），物品画像数据库压力非常大（每次读几千个物品）

  * 工程实现时，用户画像中的特征可以很多很大，但尽量不往物品画像中塞很大的向量
* 由于用户和物品画像较为静态，甚至可以把用户和物品画像缓存在排序服务器本地，加速读取（统计数据不可以缓存，因为会实时变动）
* 收集了排序所需特征后，将特征打包发给 TF Serving，Tensorflow 给笔记打分并把分数返回排序服务器
* 排序服务器依据融合的分数、多样性分数、业务规则等给笔记排序，并把排名最高的几十篇返回主服务器

## 粗排

前面介绍的模型主要用于精排，本节介绍怎么做粗排。

### 粗排 vs 精排

* 粗排

  * 给几千篇笔记打分
  * 单次推理代价必须小
  * 预估的准确性不高
* 精排

  * 给几百篇笔记打分
  * 单次推理代价很大
  * 预估的准确性更高

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709142333-2iizswi.png "精排模型")

* 精排模型

  * **前期融合**：先对所有特征做 concatenation，再输入神经网络

    * 这个网络叫 shared bottom，意思是它被多个任务共享
  * 线上推理代价大：如果有 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709142403-s4xm3ey.svg) 篇候选笔记，整个大模型要做 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709142403-rs3v23t.svg) 次推理

![1676618897140-fdd175e1-a329-48cd-ae7b-15bf2e0b20d4](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-1676618897140-fdd175e1-a329-48cd-ae7b-15bf2e0b20d4-20240709142403-t2q9b16.png "双塔模型")

* 双塔模型（一种粗排模型）

  * **后期融合**：把用户、物品特征分别输入不同的神经网络，不对用户、物品特征做融合
  * 线上计算量小：

    * 用户塔只需要做一次线上推理，计算用户表征 a
    * 物品表征 b 事先储存在向量数据库中，物品塔在线上不做推理
  * 后期融合模型不如前期融合模型准确

    * 预估准确性不如精排模型
    * 后期融合模型用于召回，前期融合模型用于精排

### 粗排的三塔模型

小红书粗排用的三塔模型，效果介于双塔和精排之间。

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709142544-58dg7yi.png)

* 交叉特征：用户特征与物品特征做交叉
* 对 3 个塔输出的向量做 Concatenation 和 Cross（交叉）得到 1 个向量
* 与前期融合在最开始对各类特征做融合不同，三塔模型在塔输出的位置做融合

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709142603-hnf5vpv.png)

![image](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/image-20240709142627-y09803p.png)

* 有 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709142723-9l0ousj.svg) 个物品，模型上层需要做 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709142723-adn7zl6.svg) 次推理
* 粗排推理的大部分计算量在模型上层

  * 这个环节无法利用缓存节省计算量
  * 三塔模型节省的是对物品推理的计算量

* 三塔模型的推理

  * 从多个数据源取特征：

    * 1 个用户的画像、统计特征
    * ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709142723-4cli4w2.svg) 个物品的画像、统计特征
  * 用户塔：只做 1 次推理
  * 物品塔：未命中缓存时需要做推理
  * 交叉塔：必须做 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709142724-fiq2j3i.svg) 次推理
  * 上层网络做 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709142724-w34argr.svg) 次推理，给 ![latex](./%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.assets/net-img-latex-20240709142724-cvne3wi.svg) 个物品打分
* 粗排模型的设计理念就是尽量减少推理的计算量，使得模型可以线上对几千篇笔记打分

# 特征交叉

> * [📎Factorized Machine FM.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1679835255118-2bdcbf65-8474-49c2-b1c6-9567c4f29741.pdf)
> * [📎深度交叉网络（DCN）.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1679835271882-fb50371a-38fe-4784-813e-eeefcfe743cc.pdf)
> * [📎LHUC 网络结构.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1679835260115-facbf8ab-b1b1-441e-abe7-9baa6d01e731.pdf)
> * [📎SENet Bilinear Cross.pdf](https://www.yuque.com/attachments/yuque/0/2023/pdf/101969/1679835260120-75710e52-baec-407f-acfd-d896c643db01.pdf)

## Factorized Machine (FM)

FM 以前常用，现在用得少了，主要了解一下思想。

###
