---
title: 第一章 分布式锁
tags:
  - 分布式锁
categories:
  - 分布式
date: '2025-01-13'
description: 欢迎使用 Curve 主题，这是你的第一篇文章
articleGPT: 这是一篇初始化文章，旨在告诉用户一些使用说明和须知。
#cover: "/images/logo/logo.webp"
---

# 分布式锁

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250402195430669.png" alt="image-20250402195430669" style="zoom: 30%;" />

## 从扣减库存案例 深入剖析共享资源的数据

### 需求背景

电商项目中，用户购买商品后，会对商品的库存进行扣减。

一张简单的商品库存表如下：

```sql
CREATE TABLE `tb_goods_stock`  (
  `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键id',
  `goods_id` bigint(20) NOT NULL COMMENT '商品id',
  `stock` int NOT NULL COMMENT '库存数',
  PRIMARY KEY (`id`)
) COMMENT = '商品库存表';
```

接着，我们创建一个`SpringBoot`的项目,在接口中实现简单的扣减库存的逻辑，示例如下：

```java
public String reductStock(Long goodsId,Integer count){
    //1.查询商品库存的库存数量
    Integer stock = stockDao.selectStockByGoodsId(goodsId);
    //2.判断商品的库存数量是否足够
    if (stock < count) return "库存不足";
    //3.如果足够，扣减库存数量
    stockDao.updateStockByGoodsId(goodsId,stock-count);
    //4.返回扣减成功
    return "库存扣减成功！";
}
```

### 发现问题

上面的例子如果是通过单次访问，那么它的执行结果也是符合我们预期的。但在高并发场景下，多个线程同时访问同一个数据就可能出现**超卖问题**。因此，我们用`JMeter`来模拟大量并发数据来进行线上抢购场景复现，如下：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250402195703413.png" alt="image-20250402195703413" style="zoom:50%;" />

添加一个线程组，设定50个线程和100次循环次数，如下：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250402195714703.png" alt="image-20250402195714703" style="zoom:50%;" />

 这时再将数据库里的商品id为1的数据的库存修改为`5000`

接着执行HTTP请求，如下：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250402195748454.png" alt="image-20250402195748454" style="zoom:50%;" />

 通过聚合报告可以看出5000次请求都执行成功，这个时候按照正常逻辑，库存应该扣完了，回到数据库查询，发现还有4000多个库存，带换到线上场景，这个时候后续还有用户继续请求购买，最终实际卖出的肯定会远远超过库存，这就是经典的**超卖问题**。

### JVM锁初显神通

`并发问题去找锁`这个几乎是大家的共识，那么这里的**超卖问题**也不例外。因此，最直接的办法就是直接在涉及扣减库存的逻辑或操作上进行`加锁`处理。首先，最先想到的就是JVM锁，只需要一个`synchronized`关键字就可以实现，代码修改如下：

```java
public synchronized String reductStock(Long goodsId,Integer count){
    //1.查询商品库存的库存数量
    Integer stock = stockDao.selectStockByGoodsId(goodsId);
    //2.判断商品的库存数量是否足够
    if (stock < count) return "库存不足";
    //3.如果足够，扣减库存数量
    stockDao.updateStockByGoodsId(goodsId,stock-count);
    //4.返回扣减成功
    return "库存扣减成功！";
}
```

我们这时候去把数据库的库存还原下，然后重新用`JMeter`进行请求（Ps:原参数不变），执行后我们先看数据库结果，这次的库存就被扣减完了，这个没有问题了。

但我们查看聚合报告会发现对比前面的请求，有一项指标下降了很多-吞吐量，从三千多到现在的一千多，所以加锁肯定对性能是会产生影响的，如下：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250402195857036.png" alt="image-20250402195857036" style="zoom: 50%;" />

当然除了`synchronized`关键字，还有更为灵活的方式，毕竟它是作用在方法上的，而我们使用`reentrantLock`则可以实现对代码块进行加锁，如下：

```java
ReentrantLock reentrantLock = new ReentrantLock();

public String reductStock(Long goodsId,Integer count){
    //1.加锁
    reentrantLock.lock();
    try {
        //2.查询商品库存的库存数量
        Integer stock = stockDao.selectStockByGoodsId(goodsId);
        //3.判断商品的库存数量是否足够
        if (stock < count) return "库存不足";
        //4.如果足够，扣减库存数量
        stockDao.updateStockByGoodsId(goodsId,stock-count);
    } finally {
        //5.解锁
        reentrantLock.unlock();
    }
    //6.返回扣减成功
    return "库存扣减成功！";
}
```

### JVM锁失效问题

经过了上面的简单改造就让我们的扣减库存不失效了，那么是否这样就可以真正地解决线上的超卖问题呢？当然不是的，JVM锁并不是万能的，它在部分场景下是会失效的，如下：

#### 1. 多例模式

首先，我们都知道Spring默认是单例的，即每个对象都会被注册成为一个bean交给IOC容器进行管理。但是它是可以设置成多例的，只需要一个简单的注解，如下：

```java
@Scope(value = "prototype", proxyMode = ScopedProxyMode.TARGET_CLASS)
@Service
public class StockService {

    @Autowired
    private StockDao stockDao;

    public synchronized String reductStock(Long goodsId,Integer count){
        //1.查询商品库存的库存数量
        Integer stock = stockDao.selectStockByGoodsId(goodsId);
        //2.判断商品的库存数量是否足够
        if (stock < count) return "库存不足";
        //3.如果足够，扣减库存数量
        stockDao.updateStockByGoodsId(goodsId,stock-count);
        //4.返回扣减成功
        return "库存扣减成功！";
    }
}
```

这个时候我们再次进行调用测试，结果查询库存还剩3800多。

那么这是为什么呢？其实很好理解，多例模式下这个类对应的`bean`也可以有多个，也就是我们每次执行到这个方法都是一个新的`bean`，自然就根本没有锁住。

#### 2. 事务模式

事务模式就是在方法上加上事务注解（Ps：这里测试记得把上面的多例注解注释掉），代码如下：

```java
@Transactional
public synchronized String reductStock(Long goodsId,Integer count){
    //1.查询商品库存的库存数量
    Integer stock = stockDao.selectStockByGoodsId(goodsId);
    //2.判断商品的库存数量是否足够
    if (stock < count) return "库存不足";
    //3.如果足够，扣减库存数量
    stockDao.updateStockByGoodsId(goodsId,stock-count);
    //4.返回扣减成功
    return "库存扣减成功！";
}
```

再次进行调用测试，可以看到依然会有剩余库存，那么为什么加上事务就破坏了JVM锁呢？

其实也很好理解：我们看代码，在扣减库存的方法上我们加了事务，方法内部加了锁，可以理解成事务包着锁。那么当请求A执行到扣减库存的方法后，会先进入事务，然后加锁->执行业务逻辑->解锁。

**这里需要注意的是**，一旦解锁之后，请求B就会马上抢夺锁，所以这个时候就出现了旧请求还没提交事务，新请求就拿到锁开始执行了。在读已提交这个默认的隔离级别下，就可能出现新旧请求扣减了同一份库存，自然**超卖问题**就又出现了。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250403093858346.png" alt="image-20250403093858346" style="zoom:100%;" />

那么是否有解决办法呢？

答案是肯定的。这里我们分析了失效的原因，那么其实只要把锁加到事务外，确保事务提交了才释放锁就行。比如按照我们现有的例子，把`synchronized`关键字加到`controller`层就行了，这里很简单就不演示了，感兴趣的读者可以自行测试。

#### 3. 集群模式

集群模式则是最常见的情况，毕竟应该不会有生产级别的服务只部署一个实例，几乎都是部署多实例的。那么这个时候JVM锁自然就失效了，如下：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250402195958814.png" alt="image-20250402195958814" style="zoom:80%;" />

在这个例子中，外部的请求进入到nginx，通过负载均衡策略转发到库存服务，JVM锁只在所在的JVM内部失效，所以这里加的JVM锁其实是3个服务各加了一把锁，那各自锁各自的等于没锁，超卖问题自然就又出现了。

## 认识分布式锁

首先，先来看它的概念-**控制分布式系统之间同步访问共享资源的一种方式**。

所以，它需要满足以下四个特性：**`互斥性`**、**`可重入性`**、**`锁超时防死锁`**、**`锁释放正确防误删`**。

而上面提到的JVM锁在分布式场景中就会存在问题，比如，我们当前有两个服务实例，它们都访问商品库存表进行扣减库存，如果使用JVM锁，其实并没有效果，如图：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250403123636049.png" alt="image-20250403123636049" style="zoom:50%;" />

JVM锁只能锁所在服务的实例，所以在分布式场景下，有多少个服务实例自然也会存在多少个JVM锁。那么有解决办法吗？当然是有的。没有什么是加一层解决不了的，我们只需要在服务实例和数据库之间再加一层作为分布式锁即可，如图：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250403124323537.png" alt="image-20250403124323537" style="zoom:50%;" />

我们可以依靠中间件来实现加的这一层，常见的有**reids**、**Zookeeper**、**Etcd**等。

### 基于Redis实现的分布式锁

在开始实现前，我们先来聊聊为什么选择**redis**来实现分布式锁。这里做技术选型，自然离不开对中间件本身的特点进行分析，**redis**的以下特点足够支持它来实现分布式锁：

- Redis是高性能的内存数据库，满足高并发的需求；
- Redis支持原子性操作，保证操作的原子性和一致性；
- Redis支持分布式部署，支持多节点间的数据同步和复制，从而满足高可用性和容错性。

除了上述特性，redis客户端提供的一个命令让我们设置锁也变得更为简单，即**setnx**，区别于**set**命令，使用它来设置键值对，如果键已存在，就不会设置成功。所以使用这个命令来获取锁的话，我们可以省去很多判断逻辑。

#### redis实现简化版分布式锁（SETNX）

首先，使用**redisTemplate**来实现下加锁和解锁的方法。加锁就是用`setnx`命令设置个键值对，`key`根据业务场景设置，`value`随意；解锁就是根据`key`删除指定的键值对，如下：

```java
@Override
public void lock() {
    //1.使用setnx指令进行加锁
    while (true) {
        Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockName, "1");
        if (result != null && result) {
          break;
        }
    }
}

@Override
public void unlock() {
    stringRedisTemplate.delete(this.lockName);
}
```

接着我们启动熟悉的**JMeter**来进行测试，如下：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250403165408855.png" alt="image-20250403165408855" style="zoom:40%;" />

这个执行效率如果放到线上肯定是不行的，前面也讲过我们选择redis是奔着高性能去的，可是为什么表现却这么差呢？我们看下加锁的逻辑，如下：

```java
public void lock() {
    //1.使用setnx指令进行加锁
    while (true) {
        Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockName, "1");
        if (result != null && result) {
          break;
        }
    }
}
```

我们这里的加锁逻辑是只要没获取到锁就去重试，而redis的写命令执行的也比较快，所有这里在高并发场景下就变成了低效重试，那么有没有解决办法呢？当然是有的，很简单，我们只需要在获取失败后，让当前线程先停一下即可，如下：

```java
public void lock() {
    //1.使用setnx指令进行加锁
    while (true) {
        Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockName, "1");
        if (result != null && result) {
          break;
        }
        try {
            Thread.sleep(50);
        } catch (InterruptedException e) {
            throw new RuntimeException(e);
        }
    }
}
```

##### 存在的问题

在上面的代码里，我们基于redis手撸了一个简化版的分布式锁，那么它是否就满足日常业务使用了呢？

当然不行，既然是简化版的自然就存在问题。我们先来分析一下前文中提到的四个特性中的其中两个-**`锁超时防死锁`**和**`锁释放正确防误删`**，那么我们的简化版能否满足呢？显然是不行的，因此就需要我们继续迭代了。

###### 1. 锁超时怎么办？

锁超时的情况可能有很多，比如扣减库存获取锁之后代码执行到一半服务挂掉了，由于是异常关闭，所以finally中释放锁的逻辑也没来得及执行，这个时候锁就被永久的持有了。

所以为了解决这个问题，我们就需要为锁加上过期时间，这样可以保证无论业务或者服务是否出现异常，最终都可以保证锁的释放，代码如下：

```java

private final long defaultExpireTime = 30000;


@Override
public void lock() {
    lock(TimeUnit.MILLISECONDS, defaultExpireTime);
}

@Override
public void lock(TimeUnit timeUnit, Long expireTime) {
    //1.使用setnx指令进行加锁
    while (true) {
        Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockName, "1", expireTime, timeUnit);
        if (result != null && result) {
            break;
        }

        try {
            Thread.sleep(100);
        } catch (InterruptedException e) {
            throw new RuntimeException(e);
        }
    }
}
```

所以其实很简单，只需要给锁加个过期时间就可以了，这个时间根据自己的业务场景定。因为如果你定的少了，假如我们定的过期时间是500毫秒，但是相应的业务逻辑执行完成需要800毫秒，那么就会造成业务逻辑还没执行完成，锁就被释放了，这锁就是加了个寂寞。

###### 2. 锁被误删了怎么办？

首先，我们来定义下什么叫锁误删，`即某个线程持有的锁被别的线程删了`。

那么这里肯定就有同学疑惑了，按照我们上面的代码逻辑，假设现在有个A线程获取到锁了，在它没释放的情况下，其他线程应该是一直循环获取才对，也就是说这个时候其他线程根本就拿不到这把锁，又怎么能给它释放了呢。

其实问题就出在我们上面为了解决锁超时问题而给锁加了过期时间，我们假设A线程的业务逻辑处理的时间超过了锁超时释放的时间，就造成了A线程还没执行完，锁就自己释放了，这个时候B线程获取到了锁开始执行，而A线程继续执行到了释放锁的逻辑。

过程如下：

1. 客户端 1 获取锁成功。
2. 客户端 1 在某个操作上阻塞了很长时间。
3. 过期时间到了，锁自动释放了。
4. 客户端 2 获取到了对应同一个资源的锁。
5. 客户端 1 从阻塞中恢复过来，释放掉了客户端 2 持有的锁。
6. 另外线程客户端 3 此时可以成功请求到锁

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250403171552701.png" alt="image-20250403171552701" style="zoom:80%;" />

那么知道了问题所在，我们怎么解决呢？很简单，只需要在释放锁之前判断下当前释放锁的线程是否是拿到锁的线程不就好了，只有一致的情况下才可以释放锁，代码如下：

```java
@Override
public void lock(TimeUnit timeUnit, Long expireTime) {
    //1.使用setnx指令进行加锁
    while (true) {
        Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockName, uuid, expireTime, timeUnit);
        if (result != null && result) {
            break;
        }

        try {
            Thread.sleep(100);
        } catch (InterruptedException e) {
            throw new RuntimeException(e);
        }
    }
}

@Override
public void unlock() {
    //1.判断当前持有锁线程是否等于本线程
    String result = stringRedisTemplate.opsForValue().get(this.lockName);
    if (this.uuid.equals(result)) {
        stringRedisTemplate.delete(this.lockName);
    }
}
```

我们这里的做法是在获取锁的时候给 **`value`** 设置一个 `uuid`，并在删除之前先判断`当前线程的uuid`和锁对应的`uuid`是否一致。

#### 通过Lua脚本保证redis操作的原子性

##### 实现思路

- 加锁

```lua
if redis.call('EXISTS', KEYS[1]) == 0 then
    redis.call('SET', KEYS[1], ARGV[1])
    redis.call('EXPIRE', KEYS[1], ARGV[2])
    return 1
else
    return 0
end
```

逻辑看起来非常的简单，就是判断一下当前是否存在这把锁，如果存在，则加锁失败；如果不存在，就set一个锁，并且给一个过期时间。脚本编写好后，就可以用**eval**来执行了，如下：

```lua
redis-cli eval "if redis.call('EXISTS', KEYS[1]) == 0 then redis.call('SET', KEYS[1], ARGV[1]); redis.call('EXPIRE', KEYS[1], ARGV[2]); return 1; else return 0; end" 1 lockName uuid 3000
```

- 解锁

```lua
if (redis.call('EXISTS', KEYS[1]) == 0) then
    return 0;
end
if (redis.call('GET', KEYS[1]) == ARGV[1]) then
    redis.call('DEL', KEYS[1])
    return 1;
else
    return 0;
end
```

逻辑也是相当的简单，就是先判断锁是否存在，如果存在，再比较的value的uuid是否一致，如果一致，则删除锁。执行如下：

- 代码中调用lua

脚本编写完成后，我们就需要在代码中进行调用，这里我们对原来的加锁和解锁方法进行改造，代码如下：

```java
    @Override
    public void lock(TimeUnit timeUnit, Long expireTime) {

        while (true) {
            // 使用Lua脚本进行加锁
            String luaScript = "if(redis.call('exists', KEYS[1]) == 0) then redis.call('set', KEYS[1], ARGV[1]) redis.call('expire', KEYS[1], ARGV[2]) return 1; else return 0; end";
            Long result = stringRedisTemplate.execute(new DefaultRedisScript<>(luaScript, Long.class),
                    Collections.singletonList(this.lockName), uuid, expireTime.toString());
            if (result != null && result.equals(1L)) {
                break;
            }

            try {
                Thread.sleep(100);
            } catch (InterruptedException e) {
                throw new RuntimeException(e);
            }
        }
    }

    @Override
    public void unlock() {
        // 使用Lua脚本进行解锁
        String luaScript = "if (redis.call('EXISTS', KEYS[1]) == 0) then return 0; end if (redis.call('GET', KEYS[1]) == ARGV[1]) then redis.call('DEL', KEYS[1]) return 1; else return 0; end";
        stringRedisTemplate.execute(new DefaultRedisScript<>(luaScript, Long.class),
                Collections.singletonList(this.lockName), uuid);
    }
```

它的使用也是非常简单，把脚本声明之后，直接通过execute方法进行传参调用即可。通过Lua脚本的加解锁主要是将原本使用redis的多步操作合并成了一步来保证了操作的原子性。

#### redis分布式锁的细节问题

##### 锁重入问题

我们讲到分布式锁具备的几个特性中有提到**可重入性**，这个特性对于分布式锁的实现至关重要。首先，我们明确下它的定义-**`在同一个线程中，同一个锁可以被多次获取而不会发生死锁`**。

假设方法A调用了需要相同锁的方法B或者本身就是递归的，当不可重入时，那么第二次获取锁的时候就会被阻塞，从而发生死锁。而观察下我们前面写的Lua脚本显然不足以支持可重入，因此我们就需要改造。

那么实现可重入的关键就是：获取了多少把锁就得解锁的时候解多少把，这里需要**保持最终一致性**。所以我们这里的实现思路就需要用到**redis**的一种数据结构-**hash**，代码如下：

```lua
if (redis.call('EXISTS', KEYS[1]) == 0) then
    redis.call('HINCRBY', KEYS[1], ARGV[1], 1)
    redis.call('PEXPIRE', KEYS[1], ARGV[2])
    return 1;
end
if (redis.call('HEXISTS', KEYS[1], ARGV[1]) == 1) then
    redis.call('HINCRBY', KEYS[1], ARGV[1], 1)
    redis.call('PEXPIRE', KEYS[1], ARGV[2])
    return 1;
else
    return 0;
end
```

这里的实现逻辑也是很简单：先判断锁是否存在，如果不存在，直接加锁，重入次数设置为1以及加过期时间；如果存在，则比较uuid是否是本线程，如果是，那么可重入次数+1，并且给锁加一个过期时间，如果不是，那么就加锁失败。那么我们的解锁逻辑也就很清晰了，如下：

```lua
if (redis.call('HEXISTS', KEYS[1], ARGV[1]) == 0) then
    return 0;
end
local lockCount = redis.call('HINCRBY', KEYS[1], ARGV[1], -1)
if (lockCount > 0) then
    redis.call('PEXPIRE', KEYS[1], ARGV[2])
    return 1;
else
    redis.call('DEL', KEYS[1])
end
```

这里的实现逻辑是：首先判断当前持有锁的线程是不是本线程，不是的话，就不需要释放了。如果是，就对重入次数减1，减1之后判断值是否大于0，如果大于还持有锁，就设置一个新的过期时间，如果不大于0，就可以删除锁了。

##### 锁续期问题

> **先来回顾一下：** 我们前面为了解决锁因异常情况（例如执行完加锁逻辑服务宕机了）未执行到释放，从而造成锁一直被占用的情况。而为了解决这个问题，我们给每个锁加上了过期时间。
>
> 但是这又引申出了新的问题：如果锁到期了，而业务还没执行完，此时就给释放了，锁又被新的线程拿到了，那么就又会产生并发问题了。
>
> 所以，我们是不希望锁在一定时间后自动过期掉的。那么，为了解决这个问题，我们应该在线程拿到锁后一直延长过期时间，直到业务执行完成后才释放这把锁。

我们分析下可以怎么做：

###### 1. 单独起个服务来处理

我们可以单独起个服务来负责为锁续期，但是这有什么问题呢？如果加锁的服务挂掉了，这个独立的客户端如何感知，如果感知不到，就会一直给锁续期。

###### 2. 获取锁进程自己续期

我们还可以获取当前进程来进行锁续期，这样做就算锁挂掉了，续期的进程也随之结束了。进程A自己如果去实现，它需要一边执行业务逻辑，一边又要进行锁续期，那么我们单独起个线程去做这件事就很合适了。

###### 3. 异步线程解决锁续期

首先，我们需要编写自动续期的Lua脚本，如下：

```lua
if (redis.call('HEXISTS', KEYS[1], ARGV[1])) then
    return 0;
else
    redis.call('PEXPIRE', KEYS[1], ARGV[2])
    return 1;
end
```

这个脚本的逻辑很简单，相信各位同学已经明白了。就是先获取锁，获取成功之后则重新设置过期时间。我们来写下代码，如下：

```java
    @Override
    public void lock(TimeUnit timeUnit, Long expireTime) {
        // 设置锁的过期时间
        this.expireTime = expireTime;
        // 循环尝试获取锁
        while (true) {
        // 检查并设置分布式锁
        // 1. 如果锁不存在,创建锁并设置过期时间
        // 2. 如果当前线程已持有锁,则重入并更新过期时间
        // 3. 如果其他线程持有锁,返回0表示获取失败
            String luaScript = "if(redis.call('exists', KEYS[1]) == 0) then redis.call('hincrby', KEYS[1], ARGV[1], 1) redis.call('pexpire', KEYS[1], ARGV[2]) return 1; end if (redis.call('hexists',KEYS[1], ARGV[1]) == 1) then redis.call('hincrby', KEYS[1], ARGV[1], 1) redis.call('pexpire', KEYS[1], ARGV[2]) return 1; else return 0; end";
            // 执行Lua脚本
            Long result = stringRedisTemplate.execute(new DefaultRedisScript<>(luaScript, Long.class), Collections.singletonList(this.lockName),
                    uuid,
                    expireTime.toString());
            // 获取锁成功
            if (result != null && result.equals(1L)) {
                // 启动守护线程,定期延长锁的过期时间
                new Thread(() -> {
                    while (true) {
                        // Lua脚本:检查并延长锁的过期时间
                        // 如果锁仍然存在且被当前线程持有,则延长过期时间
                        String expireLua = "if(redis.call('hexists', KEYS[1], ARGV[1]) == 0) then return 0; else redis.call('pexpire', KEYS[1], ARGV[2]) return 1; end";
                        Long expireResult = stringRedisTemplate.execute(new DefaultRedisScript<>(expireLua, Long.class)
                                , Collections.singletonList(this.lockName),
                                uuid,
                                expireTime.toString());
                        // 如果锁不存在或已经不属于当前线程,退出守护线程
                        if (expireResult == null || expireResult.equals(0L)) {
                            break;
                        }
                        try {
                            // 休眠时间为过期时间的一半,定期唤醒执行延期
                            Thread.sleep(expireTime / 2);
                        } catch (InterruptedException e) {
                            throw new RuntimeException(e);
                        }
                    }
                }).start();
                break;
            }
            try {
                // 获取锁失败,等待50ms后重试
                Thread.sleep(50);
            } catch (InterruptedException e) {
                throw new RuntimeException(e);
            }
        }
    }
```

这里把上章中的可重入Lua脚本替换我们的加锁逻辑，并通过另一个线程来不断地为锁进行续期，这里我们借鉴了Redisson的**看门狗机制**，在后续的章节中也会讲解到，这里关注我们的实现逻辑即可。

##### 锁阻塞问题

看下我们上面的实现：如果获取锁失败，就会在睡50ms继续轮询，直到获取锁成功为止。这个实现相当于把锁阻塞住了，在某些业务场景下，我们其实需要在某个时间内，如果获取锁失败，就放弃，需要重新请求，比如秒杀抢购就是这样的。

所以我们这里就需要在原本的基础上添加一个锁获取的超时时间，以此来解决锁阻塞问题，代码如下：

```java
public boolean tryLock(long time, long expireTime, TimeUnit unit) throws InterruptedException {
    // 记录开始尝试获取锁的时间戳
    long startTime = System.currentTimeMillis();
    // 记录当前时间戳
    long currentTime = System.currentTimeMillis();
    boolean lockResult = false;
    
    // 在指定的等待时间内循环尝试获取锁
    // time表示最大等待时间,超过这个时间还未获得锁就返回false
    while (currentTime - startTime <= time) {
        // 尝试获取锁
        boolean result = tryLockInternal(unit, expireTime);
        if (result) {
            // 获取锁成功,记录结果并退出循环
            lockResult = result;
            break;
        }
        // 更新当前时间戳
        currentTime = System.currentTimeMillis();
    }
    return lockResult;
}
```

那么在扣减库存的业务代码那里也需要加上这个tryLock的逻辑，代码如下：

```java
public String deductStockRedisLock(Long goodsId, Integer count) {
    AbstractLock lock = null;
    try {
        // 创建基于Redis的分布式锁,锁的key为"lock"+商品ID
        lock = new RedisLock(template, "lock" + goodsId);
        // 尝试在5秒内获取锁
        boolean result = lock.tryLock(5000, TimeUnit.MILLISECONDS);
        
        if (result) {
            // 获取锁成功,执行库存扣减逻辑
        }
        
        // 获取锁超时的处理
        System.out.println("获取锁超时");
        return "系统繁忙";
        
    } catch (InterruptedException e) {
        throw new RuntimeException(e);
    } finally {
        // 确保在finally块中释放锁
        if (lock != null) {
            lock.unlock();
        }
    }
}
```

这里的**Lock**和**tryLock**的使用，大家根据自己的业务需要选择性使用即可。

#### redisson框架（实现锁的优雅续期）

##### 什么是redisson

Redisson 中的分布式锁自带自动续期机制，使用起来非常简单，原理也比较简单，其提供了一个专门用来监控和续期锁的 Watch Dog（ 看门狗），如果操作共享资源的线程还未执行完成的话，Watch Dog 会不断地延长锁的过期时间，进而保证锁不会因为超时而被释放。

##### SpringBoot整合redisson

我们继续以扣减库存的案例为例，看看如何通过SpringBoot来整合`redisson`，从而解决并发问题。

###### 1. 引入依赖

在`redisson`的Github的readme里我们可以找到它的[Quick start](https://link.juejin.cn/?target=https%3A%2F%2Fredisson.org%2Fdocs%2Fgetting-started%2F)，里面就有最新版本的引入指引，当然你也可以在maven的中央仓库中找到。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250404163804880.png" alt="image-20250404163804880" style="zoom:40%;" />

###### 2. 配置对象

```java
import org.redisson.Redisson;
import org.redisson.api.RedissonClient;
import org.redisson.config.Config;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class RedissonConfig {

    @Bean
    public RedissonClient redissonClient(){
        Config config = new Config();
        config.useSingleServer().setAddress("redis://127.0.0.1:6379");
        return Redisson.create(config);
    }
    
}
```

###### 3. 代码实现

```java

@Autowired
private RedissonClient redissonClient;


public String deductStockRedissonLock(Long goodsId, Integer count) {
    RLock lock = null;
    try {
        lock = redissonClient.getLock("lock" + goodsId);
        lock.lock();
        //1.查询商品库存数量
        String stock = template.opsForValue().get("stock" + goodsId);
        if (StringUtil.isNullOrEmpty(stock)) {
            return "商品不存在！";
        }
        int lastStock = Integer.parseInt(stock);
        //2.判断库存数量是否足够
        if (lastStock < count) {
            return "库存不足！";
        }
        //3.如果库存数量足够，则去扣减库存
        template.opsForValue().set("stock" + goodsId, String.valueOf(lastStock - count));
        return "扣减库存成功";
    } catch (Exception e) {
        throw new RuntimeException(e);
    } finally {
        if (lock != null) {
            lock.unlock();
        }
    }
}
```

##### Redisson分布式锁原理

- 可重入：利用 `hash`结构记录 `线程id` 和 `重入次数`
- 可重试：利用`信号量` 和 `PubSub` 功能实现等待、唤醒，获取锁失败的重试机制
- 超时续约：利用`watchDog`，每隔一段时间（releaseTime)，重置超时时间，解决了「锁过期释放，业务没执行完」问题

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250421172306340.png" alt="image-20250421172306340" style="zoom:80%;" />

#### RedLock分布式锁（多机版）

前面几种方案都只是基于单机版的讨论，还不是很完美。其实 Redis 一般都是集群部署的：

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250421172717218.png" alt="image-20250421172717218" style="zoom:50%;" />

如果线程一在 Redis 的 master 节点上拿到了锁，但是加锁的 key 还没同步到 slave 节点。恰好这时，master 节点发生故障，一个 slave 节点就会升级为 master 节点。线程二就可以获取同个 key 的锁啦，但线程一也已经拿到锁了，锁的安全性就没了。

为了解决这个问题，Redis 作者 antirez 提出一种高级的分布式锁算法：Redlock。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250421172803708.png" alt="image-20250421172803708" style="zoom:50%;" />

> - 1. 获取当前时间，以毫秒为单位。
> - 1. 按顺序向 5 个 master 节点请求加锁。客户端设置网络连接和响应超时时间，并且超时时间要小于锁的失效时间。（假设锁自动失效时间为 10 秒，则超时时间一般在 5-50 毫秒之间,我们就假设超时时间是 50ms 吧）。如果超时，跳过该 master 节点，尽快去尝试下一个 master 节点。
> - 1. 客户端使用当前时间减去开始获取锁时间（即步骤 1 记录的时间），得到获取锁使用的时间。当且仅当超过一半（N/2+1，这里是 5/2+1=3 个节点）的 Redis master 节点都获得锁，并且使用的时间小于锁失效时间时，锁才算获取成功。（如上图，10s> 30ms+40ms+50ms+4m0s+50ms）
> - 1. 如果取到了锁，key 的真正有效时间就变啦，需要减去获取锁所使用的时间。
> - 1. 如果获取锁失败（没有在至少 N/2+1 个 master 实例取到锁，有或者获取锁时间已经超过了有效时间），客户端要在所有的 master 节点上解锁（即便有些 master 节点根本就没有加锁成功，也需要解锁，以防止有些漏网之鱼）。

简化下步骤就是：

- 搞多个 Redis master 部署，以保证它们不会同时宕掉
- 获取当前时间，以毫秒为单位
- 按顺序向 5 个 master 节点请求加锁
- 根据设置的超时时间来判断，是不是要跳过该 master 节点
- 如果大于等于 3（N/2+1）个节点加锁成功，并且使用的时间小于锁的有效期，即可认定加锁成功啦，那么客户端应该立即向所有 Redis 节点发起释放锁的操作（即前面介绍的 Redis Lua 脚本）
- 如果获取锁失败，解锁

> 注意！！！redLock 会直接连接多个 Redis 主节点，不是通过集群机制连接的。
>
> RedLock 的写与主从集群无关，直接操作的是所有主节点，所以才能避开主从故障切换时锁丢失的问题。

`细节问题`

- 问题 1：为什么要在多个实例上加锁？

本质上为了容错，部分实例异常宕机，剩余实例只要超过 N/2+1 依旧可用。多个实例节点，实际上构建了一个分布式锁系统。分布式系统中，总会有异常节点，所以需要考虑异常节点达到多少个，也不会影响整个系统的正确性。（可以参考一下拜占庭将军问题的分析）

- 问题 2：为什么步骤 3 加锁成功之后，还要计算加锁的累计耗时？

因为加锁操作的针对的是分布式中的多个节点，所以耗时肯定是比单个实例耗时更久，至少需要 N/2+1 个网络来回，还要考虑网络延迟、丢包、超时等情况发生，网络请求次数越多，异常的概率越大。所以即使 N/2+1 个节点加锁成功，但如果加锁的累计耗时已经超过了锁的过期时间，那么此时的锁已经没有意义了。

- 问题 3：为什么释放锁，要操作所有节点，对所有节点都释放锁？

因为当对某一个 Redis 节点加锁时，可能因为网络原因导致加锁“失败”。注意这个“失败”，指的是 Redis 节点实际已经加锁成功了，但是返回的结果因为网络延迟并没有传到加锁的线程，被加锁线程丢弃了，加锁线程误以为没有成功，于是加锁线程去尝试下一个节点了。

所以释放锁的时候，不管以前有没有加锁成功，都要释放所有节点的锁，以保证清除节点上述图中发生的情况导致残留的锁。

##### 存在的问题

###### 失败重试（脑裂问题）

高并发场景下，当多个加锁线程并发抢锁时，可能导致脑裂，最终造成任何一个线程都无法抢到锁的情况。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250421173243936.png" alt="image-20250421173243936" style="zoom:50%;" />

所以当一个加锁线程无法获得锁的时候，应该在一个**随机延时**后再一次尝试获得锁。加锁线程从多数 Redis 实例中获得锁越快，出现脑裂的窗口越小（重试的次数也越少）。所以理想情况下，加锁线程应该多路复用地同时向 N 个实例发送加锁命令。

###### 崩溃恢复（AOF 持久化）

假设 Rodlock 算法中的 Redis 发生了崩溃-恢复，那么锁的安全性将无法保证。假设加锁线程在 5 个实例中对其中 3 个加锁成功，获得了这把分布式锁，这个时候 3 个实例中有一个实例被重启了。重启后的实例将丢失其中的锁信息，这个时候另一个加锁线程可以对这个实例加锁成功，此时两个线程同时持有分布式锁。锁的安全性被破坏。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250421174037546.png" alt="image-20250421174037546" style="zoom:60%;" />

如果我们配置了 AOF 持久化，只能减少它发生的概率而无法保证锁的绝对安全。断电的场景下，如果 Redis 被配置了默认每秒同步数据到硬盘，重启之后 lockKey 可能会丢失，理论上，如果我们想要保证任何实例重启的情况下锁都是安全的，需要在持久化配置中设置 fsync=always，但此时 Redis 的性能将大大打折扣。

为了保证这一点，我们只需要让一个崩溃时间、不可用时间（实例崩溃后存在的锁的所有 key 所需的时间）比最大 TTL 还要长的实例变成非法和自动释放的。

如果不配置 Redis 持久化，那么只能使用延迟重启保证锁的安全性。

> 结论：为了保证 Redlock 算法的安全性，有如下两种手段
>
> - 持久化配置中设置 `fsync=always`，性能大大降低
> - 恰当的运维，把崩溃节点进行延迟重启，超过崩溃前所有锁的 TTL 时间之后才加入 Redlock 节点组

所以在实际业务中并不推荐大家使用这套算法，这里只是做下知识扩展。

#### Redis 实现分布式锁的问题

##### 节点重启

N 个 Redis 节点中如果有节点发生崩溃重启，会对锁的安全性有影响的。具体的影响程度跟 Redis 对数据的持久化程度有关。参考上面的“崩溃恢复（AOF 持久化）。

> 【备注】在默认情况下，Redis 的 AOF 持久化方式是每秒写一次磁盘（即执行 fsync），因此最坏情况下可能丢失 1 秒的数据。为了尽可能不丢数据，Redis 允许设置成每次修改数据都进行 fsync，但这会降低性能。当然，即使执行了 fsync 也仍然有可能丢失数据（这取决于系统而不是 Redis 的实现）。所以，上面分析的由于节点重启引发的锁失效问题，总是有可能出现的。

**如何解决这个问题？**

Redis 之父 antirez 提出了延迟重启(delayed restarts)的概念。也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，这段时间应该大于锁的有效时间(lock validity time)。这样的话，这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。

##### 时钟变迁

Redlock 的安全性(safety property)对系统的时钟有比较强的依赖，一旦系统的时钟变得不准确，算法的安全性也就保证不了了。

> 结论：Redis 的过期时间是依赖系统时钟的，如果时钟漂移过大时会影响到过期时间的计算。

1. 加锁线程 1 从节点 Redis1, Redis2, Redis3 成功获取了锁（多数节点）。由于网络问题，与 Redis4、Redis5 通信失败。
2. 节点 Redis3 上的时钟发生了向前跳跃，导致它上面维护的锁快速过期。
3. Redis5 成功获取了同一个资源的锁（多数节点）。
4. 加锁线程 1 和加锁线程 2 现在都认为自己持有了锁。

**什么情况下会发生时钟变迁？**

- 人为修改了时钟
- 从 NTP 服务收到了一个大的时钟更新事件导致时钟漂移
- 闰秒（是指为保持协调世界时接近于世界时时刻，由国际计量局统一规定在年底或年中或者季末对协调世界时增加或减少 1 秒的调整，此时一分钟为 59 秒或者 61 秒，闰秒曾使许多大型系统崩溃）
- ……

**如何解决这个问题？**

1. Redis 之父 antirez 在 Redlock 论战中的解释：实际系统中是可以避免大的时钟跳跃的。当然，这取决于基础设施和运维方式。（实际上这种理想情况是很难达到的，不同的 redis 节点，毫秒级别的时间误差几乎是必然存在的。）
2. Fencing token 机制：类似 raft 算法、zab 协议中的全局递增数字，对这个 token 的校验需要后端资源进行校验，如此一来，相当于后端资源具备了互斥机制，这种情况下为什么还要一把分布式锁呢？而且涉及到后端资源的改造。

### 基于Zookeeper实现的分布式锁

利用Zookeeper的临时顺序节点和监听机制两大特性，可以帮助我们实现分布式锁。

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/ae0497ce8d0844b9a9d5aaa91985fccb.jpg" alt="1671157876838.jpg" style="zoom:60%;" />

1. 首先得有一个持久节点`/locks`, 路径服务于某个使用场景，如果有多个使用场景建议路径不同。
2. 请求进来时首先在`/locks`创建临时有序节点(`EPHEMERAL_SEQUENTIAL`)，所有会看到在`/locks`下面有seq-000000000, seq-00000001 等等节点。
3. 然后判断当前创建得节点是不是`/locks`路径下面最小的节点
   1. 如果是，获取锁，
   2. 不是，阻塞线程，同时设置监听器，监听前一个节点。

4. 获取到锁以后，开始处理业务逻辑，最后`delete`当前节点，表示释放锁。
5. 后一个节点就会收到通知，唤起线程，重复上面的判断。

> **大家有没有想过为什么要设置对前一个节点的监听？**

主要为了避免**羊群效应**。所谓羊群效应就是一个节点挂掉，所有节点都去监听，然后做出反应，这样会给服务器带来巨大压力，所以有了临时顺序节点，当一个节点挂掉，只有它后面的那一个节点才做出反应。

#### 原生Zookeeper客户端实现分布式锁

通过原生zookeeper api方式的实现，可以加强我们对zk实现分布式锁原理的理解。

```java

public class DistributedLock {
    private String connectString = "10.100.1.176:2281";
    private int sessionTimeout = 2000;
    private ZooKeeper zk;
    private String rootNode = "lock";
    private String subNode = "seq-";
    private String waitPath;
    // 当前client创建的子节点
    private String currentNode;
    private CountDownLatch waitDownLatch = new CountDownLatch(1);
    
  	public DistributedLock() throws IOException, InterruptedException, KeeperException {
        zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() {
            @Override
            public void process(WatchedEvent event) {
                //  发生了 waitPath 的删除事件
                if(event.getType() == Event.EventType.NodeDeleted && event.getPath().equals(waitPath)) {
                    waitDownLatch.countDown();
                }
            }
        });
        // 获取根节点
        Stat stat = zk.exists("/" + rootNode, false);
        // 如果根节点不存在，则创建根节点
        if(stat == null) {
            System.out.println("创建根节点");
            zk.create("/" + rootNode, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
        }
    }
    
  	public void zkLock() {
        try {
            // 在根节点创建临时顺序节点
            currentNode = zk.create("/" + rootNode + "/" + subNode, null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
            // 获取子节点
            List<String> childrenNodes = zk.getChildren("/" + rootNode, false);
            // 如果只有一个子节点，说明是当前节点，直接获得锁
            if(childrenNodes.size() == 1) {
                return;
            } else {
                //对根节点下的所有临时顺序节点进行从小到大排序
                Collections.sort(childrenNodes);
                //当前节点名称
                String thisNode = currentNode.substring(("/" + rootNode + "/").length());
                //获取当前节点的位置
                int index = childrenNodes.indexOf(thisNode);
                if (index == -1) {
                    System.out.println("数据异常");
                } else if (index == 0) {
                    // index == 0, 说明 thisNode 在列表中最小, 当前client 获得锁
                    return;
                } else {
                    // 获得排名比 currentNode 前 1 位的节点
                    this.waitPath = "/" + rootNode + "/" + childrenNodes.get(index - 1);
                    // 在 waitPath节点上注册监听器, 当 waitPath 被删除时,zookeeper 会回调监听器的 process 方法
                    zk.getData(waitPath, true, new Stat());
                    //进入等待锁状态
                    waitDownLatch.await();
                }
            }
        } catch (KeeperException e) {
            e.printStackTrace();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
    
  	public void zkUnlock() {
        try {
            zk.delete(this.currentNode, -1);
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (KeeperException e) {
            e.printStackTrace();
        }
    }
}
```

#### Curator 框架实现分布式锁

在实际的开发中，我们会直接使用成熟的框架 [Curator](https://link.juejin.cn?target=https%3A%2F%2Fcurator.apache.org%2Findex.html)客户端，它里面封装了分布式锁的实现，避免我们去重复造轮子。

1. pom.xml添加如下依赖

```xml
<dependency>
		<groupId>org.apache.curator</groupId>
		<artifactId>curator-recipes</artifactId>
		<version>5.2.1</version>
</dependency>
```

2. 通过`InterProcessLock`实现分布式锁

```java
@Configuration
public class CuratorConfig {

    @Bean
    public CuratorFramework curatorFramework(){
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(3000, 3);
        CuratorFramework curatorFramework = CuratorFrameworkFactory
                .newClient("127.0.0.1:2181", retryPolicy);
        curatorFramework.start();
        return curatorFramework;
    }
}

@Service
public class StockService {
		@Autowired
    private CuratorFramework curatorFramework;
  
  	public String deductStockCurator(Long goodsId, Integer count) {
        InterProcessMutex lock = null;
        try {
            lock = new InterProcessMutex(curatorFramework, "/" + "lock" + goodsId);
            lock.acquire();
            //1、查询商品库存的库存数量
            String stock = template.opsForValue().get("stock" + goodsId);
            if (StringUtil.isNullOrEmpty(stock)) {
                return "商品不存在";
            }
            Integer lastStock = Integer.parseInt(stock);
            //2、判断库存数量是否足够
            if (lastStock < count) {
                return "库存不足";
            }
            //3、如果库存数量足够，那么就去扣减库存
            template.opsForValue().set("stock" + goodsId, String.valueOf(lastStock - count));
            return "库存扣减成功";
        } catch (Exception e) {
            throw new RuntimeException(e);
        } finally {
            if (lock != null) {
                try {
                    lock.release();
                } catch (Exception e) {
                    throw new RuntimeException(e);
                }
            }
        }
    }
}
```

#### zk 实现分布式锁的问题

##### 时钟变迁问题

ZooKeeper 不依赖全局时间，它使用 zab 协议实现分布式共识算法，不存在该问题。

##### 超时导致锁失效问题

ZooKeeper 不依赖有效时间，它依靠心跳维持锁的占用状态，不存在该问题。

看起来这个锁相当完美，没有 Redlock 过期时间的问题，而且能在需要的时候让锁自动释放。但仔细考察的话，并不尽然。客户端可以删除锁，ZooKeeper 服务器也可以删除锁，会引发什么问题。

##### ZooKeeper 是怎么检测出某个客户端已经崩溃了呢？

实际上，每个客户端都与 ZooKeeper 的某台服务器维护着一个 Session，这个 Session 依赖定期的心跳(heartbeat)来维持。如果 ZooKeeper 长时间收不到客户端的心跳（这个时间称为 Sesion 的过期时间），那么它就认为 Session 过期了，通过这个 Session 所创建的所有的 ephemeral 类型的 znode 节点都会被自动删除。

##### zk 真正存在的问题

1. 客户端 1 创建了 znode 节点/lock，获得了锁。
2. 客户端 1 进入了长时间的 GC pause。（或者网络出现问题、或者 zk 服务检测心跳线程出现问题等等）
3. 客户端 1 连接到 ZooKeeper 的 Session 过期了。znode 节点/lock 被自动删除。
4. 客户端 2 创建了 znode 节点/lock，从而获得了锁。
5. 客户端 1 从 GC pause 中恢复过来，它仍然认为自己持有锁。

这个场景下，客户端 1 和客户端 2 在一段窗口时间内同时获取到锁。

> 结论：使用 ZooKeeper 的临时节点实现的分布式锁，它的锁安全期是在客户端取得锁之后到 zk 服务器会话超时的阈值（跨机房部署很容易出现）的时间之间。它无法设置占用分布式锁的时间，何时 zk 服务器会删除锁是不可预知的，所以这种方式它比较适合一些客户端获取到锁之后能够**快速处理完毕**的场景。

### 基于MySQL实现的分布式锁

#### 建立唯一索引

```sql
insert into record_lock (lock_name, uuid) values (#{lockName}, #{uuid})
```

- 唯一索引：`lock_name`
- uuid 的作用：防止解锁，删除的时候，其他客户端删掉不属于他的锁

##### 实现思路

- 执行insert 成功`insert into record_lock (lock_name, uuid) values (#{lockName}, #{uuid})`
- 执行delete成功`delete from record_lock where lock_name = #{lockName} and uuid = #{uuid}`

##### 存在的问题

- 这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。

`解决方案`: 其他服务可以启一个定时job，超过一定时间的锁，自动删除

- 这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错，没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作

`解决方案`: 搞一个while循环，直到insert成功再返回成功。

- 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁，因为数据中数据已经存在了

`解决方案`: 在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。

#### 悲观锁

```sql
select lock_name from distribute_lock where lock_name = #{lockName} for update
```

##### 实现思路

```java
public boolean lock() {
    // 开启事务
    connection.setAutoCommit(false);
    // 循环阻塞，等待获取锁
    while (true) {
        // 执行获取锁的sql
        result = select lock_name from distribute_lock where lock_name = #{lockName} for update;
        // 结果非空，加锁成功
        if (result != null) {
            return true;
        }
    }
    // 加锁失败
    return false;
}

/**
 * 解锁
 */
public void unlock() {
    // 提交事务，解锁
    connection.commit();
}
```

- 获取锁其实就是在select语句后增加`for update`，数据库会在查询过程中给数据库表增加排他锁。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁，我们可以认为获得排它锁的线程即可获得分布式锁。
- 释放锁通过`connection.commit();`操作，提交事务来实现。

##### 存在的问题

- 排他锁会占用连接，产生连接爆满的问题
- 如果表不大，可能并不会使用行锁

#### 乐观锁

```sql
update distribute_lock set version=version+1 where lock_name = #{lockName} and version=#version#
```

##### 实现思路

一般是通过为数据库表添加一个 `version` 字段来实现读取出数据时，将此版本号一同读出.

之后更新时，对此版本号加1，在更新过程中，会对版本号进行比较，如果是一致的，没有发生改变，则会成功执行本次操作；如果版本号不一致，则会更新失败。

实际就是个`CAS`过程。

##### 遇到的问题

- 这种操作方式，使原本一次的update操作，必须变为2次操作: select版本号一次；update一次。增加了数据库操作的次数。
- 如果业务场景中的一次业务流程中，多个资源都需要用保证数据一致性，那么如果全部使用基于数据库资源表的乐观锁，就要让每个资源都有一张资源表，这个在实际使用场景中肯定是无法满足的。而且这些都基于数据库操作，在高并发的要求下，对数据库连接的开销一定是无法忍受的。

### 基于ETCD实现的分布式锁

Etcd 是一个分布式，可靠的 Key-Value 存储系统，主要用于存储分布式系统中的关键数据。

> **Etcd命名**
>
> ◆ ETC : Unix中/etc目录用于存放系统中的配置文件。
>
> ◆ D : Distribute system 。

**Etcd vs Zookeeper**

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250422141045691.png" alt="image-20250422141045691" style="zoom:35%;" />

#### 基础机制

##### **Lease 机制**

租约机制（**TTL**，Time To Live），etcd 可以为存储的 **key-value** 对设置租约，**当租约到期，key-value 将失效删除**；

同时也支持**续约**，通过客户端可以在租约到期之前续约， 以避免 **key-value** 对过期失效。

Lease 机制可以**保证分布式锁的安全性**，为锁对应的 key 配置租约， **即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放**。

> ETCD 的租约比较特殊，可以先创建一个租约，然后绑定多个`key`，等到租约到期后，所有绑定的`key`就都会被移除。

##### **Revision 机制**

存储的每个 key 带有一个 `Revision` 号，Etcd 每进行一次事务操作，对应的全局 `Revision` 值都会加一，因此每个 key 对应的 Revision 属性值都是全局唯一的。通过比较 `Revision` 的大小就能知道写操作的顺序。

##### **Prefix 机制**

即前缀机制，可以根据前缀获取该目录下所有的 `key` 及对应的属性（包括 `key`, `value` 以及 `revision` 等）。

> 例如，一个名为 /etcd/lock 的锁，两个争抢它的客户端进行写操作， 实际写入的 key 分别为：key1="/etcd/lock/UUID1"，key2="/etcd/lock/UUID2"。
>
> 其中，UUID 表示全局唯一的 ID，确保两个 key 的唯一性。
>
> 写操作都会成功，但返回的 Revision 不一样， 那么，如何判断谁获得了锁呢？通过 **前缀** /etcd/lock 查询，返回包含两个 key-value 对的的 KeyValue 列表， 同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁。

##### **Watch 机制**

即监听机制，`Watch` 机制支持 `Watch` 某个固定的 `key`，也支持 `Watch` 一个目录（前缀机制），当被 `Watch` 的 `key` 或目录发生变化，客户端将收到通知。

> 在实现分布式锁时，如果抢锁失败，可通过 `Prefix` 机制返回的 `Key-Value` 列表获得 `Revision` 比自己小且相差最小的 `Key`（称为 `Pre-Key`），对 `Pre-Key` 进行监听，因为只有它释放锁，自己才能获得锁，如果监听到 `Pre-Key` 的 `DELETE` 事件，则说明 `Pre-Key` 已经释放，自己已经持有锁。

##### **顺序更新 机制**

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250422142049202.png" alt="image-20250422142049202" style="zoom:50%;" />

所有**节点变更**类的请求，都会转发到 **leader** 节点进行更新，以此保证数据的一致性。

#### etcd分布式锁的实现流程

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250422153430381.png" alt="image-20250422153430381" style="zoom:50%;" />

1. 建立连接

客户端连接 etcd，以 `/etcd/lock` 为前缀创建全局唯一的 `key`， 假设第一个客户端对应的 `key="/etcd/lock/UUID1"`，第二个为 `key="/etcd/lock/UUID2"`； 客户端分别为自己的 key 创建租约 - Lease，租约的长度根据业务耗时确定；

2. 创建定时任务作为租约的“心跳”

当一个客户端持有锁期间，其它客户端只能等待，为了避免等待期间租约失效， 客户端需创建一个定时任务作为“心跳”进行续约。此外，如果持有锁期间客户端崩溃， 心跳停止，key 将因租约到期而被删除，从而锁释放，避免死锁；

3. 客户端将自己全局唯一的 `key` 写入 etcd

执行 `put` 操作，将步骤 1 中创建的 `key` 绑定 `租约` 写入 Etcd，根据 Etcd 的 `Revision` 机制， 假设两个客户端 `put` 操作返回的 `Revision` 分别为 1、2，客户端需记录 `Revision` 用以 接下来判断自己是否获得锁；

4. 客户端判断是否获得锁

客户端以前缀 `/etcd/lock/` 读取 `key-Value` 列表，判断自己 `key` 的 `Revision` 是否为当前列表中 **最小的**，如果是则认为获得锁；否则监听列表中前一个 `Revision` 比自己小的 `key` 的删除事件，一旦监听到 **删除事件** 或者因 **租约失效** 而删除的事件，则自己获得锁；

5. 执行业务

获得锁后，操作共享资源，执行业务代码

6. 释放锁

完成业务流程后，删除对应的key释放锁

#### Jetcd客户端代码实现 

```java
public class EtcdLock extends AbstractLock {

    private Client client;
    private Long DEFAULT_LEASE = 20L;
    private Long leaseId;
    private String lockPath;

    public EtcdLock(Client client, String lockName) {
        this.client = client;
        this.lockName = lockName;
    }

    @Override
    public void lock() {
        //创建租约
        createLease();
        //创建锁
        createLock();
    }

    private void createLease() {
        Lease leaseClient = client.getLeaseClient();
        try {
            LeaseGrantResponse leaseGrantResponse = leaseClient.grant(DEFAULT_LEASE).get();
            this.leaseId = leaseGrantResponse.getID();

            StreamObserver<LeaseKeepAliveResponse> observer = new StreamObserver<LeaseKeepAliveResponse>() {
                @Override
                public void onNext(LeaseKeepAliveResponse leaseKeepAliveResponse) {

                }

                @Override
                public void onError(Throwable throwable) {

                }

                @Override
                public void onCompleted() {

                }
            };
            //对lease租约自动续期的api
            leaseClient.keepAlive(leaseId, observer);
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException(e);
        }
    }

    private void createLock() {
        Lock lockClient = client.getLockClient();
        try {
            LockResponse lockResponse = lockClient.lock(ByteSequence.from(this.lockName.getBytes()), this.leaseId).get();
            if (lockResponse != null) {
                this.lockPath = lockResponse.getKey().toString();
            }
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException(e);
        }
    }

    @Override
    public void unlock() {
        //第一释放锁
        if (this.lockPath != null) {
            try {
                client.getLockClient().unlock(ByteSequence.from(this.lockPath.getBytes())).get();
            } catch (InterruptedException | ExecutionException e) {
                throw new RuntimeException(e);
            }
        }
        //第二将lease进行revoke
        if (this.leaseId != null) {
            try {
                client.getLeaseClient().revoke(this.leaseId).get();
            } catch (InterruptedException | ExecutionException e) {
                throw new RuntimeException(e);
            }
        }
    }
}
```

### 分布式锁常⻅⾯试题

#### 什么是分布式锁?
分布式锁是在分布式环境中实现锁的机制。在⼀个分布式系统中，如果多个进程需要对同⼀资源进 ⾏操作，为了保证数据的⼀致性和防⽌数据竞争，就需要使⽤分布式锁。

#### 为什么要使⽤分布式锁？

- 单机锁和分布式锁都是为了保证数据的⼀致性和防⽌并发冲突，但是它们适⽤的场景不同。
- 单机锁主要⽤于单个系统或者服务中，当多个线程并发访问同⼀份资源时，通过单机锁可以保证在同⼀时间只有⼀个线程能够访问该资源，从⽽保证数据的⼀致性。
- 但是，当系统扩展到分布式环境，即多个系统或服务需要访问同⼀份资源时，单机锁就⽆法满⾜需求了，因为单机锁只能在同⼀台机器上锁住资源，⽆法跨节点⼯作。
- 分布式锁则是为了解决这个问题⽽出现的。在分布式环境下，多个系统或服务可能在不同的机器上运⾏，它们可能需要同时访问和修改同⼀份数据，如果没有合适的并发控制机制，就可能导致数据的不⼀致性。分布式锁可以跨节点对资源进⾏加锁，保证在同⼀时间只有⼀个系统或服务能够访问该资源，从⽽保证数据的⼀致性。
- 因此，当系统从单机扩展到分布式环境时，就需要使⽤分布式锁来替代单机锁，以保证数据的⼀致性和防⽌并发冲突。

#### 分布式锁应该具备哪些条件？

一个最基本的分布式锁需要满足：

- **互斥**：任意一个时刻，锁只能被一个线程持有。
- **高可用**：锁服务是高可用的，当一个锁服务出现问题，能够自动切换到另外一个锁服务。并且，即使客户端的释放锁的代码逻辑出现问题，锁最终一定还是会被释放，不会影响其他线程对共享资源的访问。这一般是通过超时机制实现的。
- **可重入**：一个节点获取了锁之后，还可以再次获取锁。

除了上面这三个基本条件之外，一个好的分布式锁还需要满足下面这些条件：

- **高性能**：获取和释放锁的操作应该快速完成，并且不应该对整个系统的性能造成过大影响。
- **非阻塞**：如果获取不到锁，不能无限期等待，避免对系统正常运行造成影响。
- ⽀持公平锁和⾮公平锁

#### 分布式锁的实现⽅式有哪些？

常⻅的实现⽅式有

- 基于数据库的分布式锁，这种实现简单，但是性能较低。
- 基于Redis实现的分布式锁，这种实现性能⾼，但是存在因为主从同步导致的安全问题。
- 基于Zookeeper的分布式锁，实现性能稍弱于Redis，可靠性安全性较⾼。
- 基于Etcd实现的分布式锁，性能不错，API使⽤简单。

#### Redis实现分布式锁怎么实现的？

⾸先实现分布式锁需要满⾜⼏个特点：

1. 互斥性
   - setnx

2. 原⼦性
   - lua

3. 锁超时，防死锁
   - 过期时间

4. 锁误删
   - 客户端uuid

5. 可重⼊性
   - hash数据结构存储

6. ⽀持阻塞和⾮阻塞
   - 设置获取锁超时时间

7. ⽀持公平锁和⾮公平锁
   - 利⽤redis的list数据结构，⽤于等待锁线程排队。

#### Redisson实现分布式锁源码？

以下是Redisson实现分布式锁的基本流程：

- 获取锁
  - 当⼀个线程请求获取锁时，Redisson内部会构建⼀个Redis的lua脚本，然后发送给Redis服务器执⾏。
  - 这个lua脚本主要做两件事情：⼀是尝试设置⼀个key（锁的名字），⼆是如果设置成功，就给这个key设置⼀个过期时间。这两个操作是原⼦的。如果设置key成功，表示获取锁成功，如果设置key失败（key已存在），表示获取锁失败。
- 锁续期
  - 为了防⽌线程在获取锁后，因为处理业务逻辑时间过⻓⽽导致锁⾃动过期，Redisson内部会启动⼀个定时任务，定期给锁续期。
- 释放锁
  - 当线程完成业务处理后，会通过Redis的lua脚本来尝试释放锁。
  - 这个lua脚本主要做两件事情：⼀是获取key的值，⼆是如果值与预期相同，就删除这个key。这两个操作也是原⼦的。这样可以保证只有获取锁的线程才能释放这个锁。

以上就是Redisson实现分布式锁的基本流程。在实际使⽤中，Redisson还提供了公平锁、读写锁、多锁、红锁等⾼级功能，可以满⾜各种复杂的分布式锁需求。

#### 了解RedLock算法么？他是解决什么问题？怎么实现？

redis分布式锁问题：

因为redis在进⾏主从复制时是异步完成的，⽐如在clientA获取锁后，主redis复制数据到从redis过程中崩溃了，导致没有复制到从redis中，然后从redis选举出⼀个升级为主redis,造成新的主redis没有clientA 设置的锁，这是clientB尝试获取锁，并且能够成功获取锁，导致互斥失效；

redLock的实现:

<img src="https://gitee.com/JBL_lun/tuchuang/raw/master/assets/image-20250426184059834.png" alt="image-20250426184059834" style="zoom:40%;" />

简化下步骤就是：

- 搞多个 Redis master 部署，以保证它们不会同时宕掉
- 获取当前时间，以毫秒为单位
- 按顺序向 5 个 master 节点请求加锁
- 根据设置的超时时间来判断，是不是要跳过该 master 节点
- 如果大于等于 3（ N/2+1）个节点加锁成功，并且使用的时间小于锁的有效期，即可认定加锁成功啦，那么客户端应该立即向所有 Redis 节点发起释放锁的操作（即前面介绍的 Redis Lua 脚本）
- 如果获取锁失败，解锁

> 注意！！！redLock 会直接连接多个 Redis 主节点，不是通过集群机制连接的。
>
> RedLock 的写与主从集群无关，直接操作的是所有主节点，所以才能避开主从故障切换时锁丢失的问题。

Redisson利⽤了MultiLock实现了RedLock，通过设置锁成功个数 > 实例个数/2来实现。

#### Zookeeper如何实现分布式锁？

- 实现互斥性
  - 利⽤Zk节点唯⼀
- 实现删除锁
  - 利⽤Zk delete删除节点
- 实现锁超时，防⽌死锁
  - 临时节点
- 实现锁唤醒
  - watch机制
- 解决惊群效应
  - 临时顺序节点

#### 15.8 Curator实现分布式锁源码？

- 创建锁：当⼀个线程尝试获取锁时，Curator会在指定的锁路径下创建⼀个临时有序节点。
- 检查是否获取到锁：Curator会获取锁路径下的所有⼦节点，然后检查刚才创建的节点是否是序号最⼩的。如果是，表示获取锁成功。如果不是，表示锁被其他线程占⽤。
- 等待锁：如果锁被其他线程占⽤，Curator会找到⽐⾃⼰序号⼩的最⼤的那个节点，然后在这个节点上注册⼀个Watcher，⽤来监听这个节点的删除事件。
- 获取到锁通知：当Watcher监听到节点被删除，Curator会再次尝试获取锁。这个过程会不断重复，直到获取到锁。
- 释放锁：当线程不再需要锁时，会删除⾃⼰创建的那个节点，从⽽释放锁。

Curator的这种实现⽅式，利⽤了Zookeeper的临时有序节点和Watcher机制，可以有效的避免死锁，并且可以实现公平锁。⽽且，由于Curator对Zookeeper的操作进⾏了封装，使得使⽤起来更加⽅便。

#### Mysql实现分布式锁怎么实现？

- 利⽤Mysql悲观锁。
- 利⽤Mysql乐观锁。
- 利⽤Mysql唯⼀索引。

#### Etcd实现分布式锁怎么实现？

- prefix 前缀查询
- revision 机制
- 租约机制
- watch机制

